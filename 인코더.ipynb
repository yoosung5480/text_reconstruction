{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sbs\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BartConfig, BartModel, PreTrainedTokenizerFast\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import BartConfig, BartModel, PreTrainedTokenizerFast\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_infilling_masking(text, mask_token=\"<mask>\", mask_prob=0.15, max_mask_size=3):\n",
    "    \"\"\"\n",
    "    Infilling Masking을 적용하는 함수.\n",
    "    \n",
    "    Args:\n",
    "        text (str): 원본 텍스트\n",
    "        mask_token (str): 마스킹 토큰 (디폴트: <mask>)\n",
    "        mask_prob (float): 토큰을 마스킹할 확률 (디폴트: 0.15)\n",
    "        max_mask_size (int): 최대 연속 마스킹 토큰 수 (디폴트: 3)\n",
    "\n",
    "    Returns:\n",
    "        str: 마스킹된 텍스트\n",
    "    \"\"\"\n",
    "    # 텍스트를 공백 기준으로 토큰화\n",
    "    tokens = text.split()\n",
    "\n",
    "    # 마스킹 대상 토큰 선택\n",
    "    num_masks = max(1, int(len(tokens) * mask_prob))\n",
    "    mask_positions = random.sample(range(len(tokens)), num_masks)\n",
    "\n",
    "    # 마스킹 적용\n",
    "    for pos in mask_positions:\n",
    "        mask_length = random.randint(1, max_mask_size)  # 연속 마스크 길이\n",
    "        tokens[pos:pos + mask_length] = [mask_token]\n",
    "\n",
    "    # 마스킹된 텍스트 반환\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'datas/test.csv'\n",
    "train_path = 'datas/train.csv'\n",
    "\n",
    "test_pd = pd.read_csv(test_path)\n",
    "train_pd = pd.read_csv(train_path)\n",
    "\n",
    "# 가장 긴 생성할 테스트 난독화 텍스트 길이 : 1965\n",
    "# 두 문장 모두 최대길이는 1381 1381\n",
    "train_text = list(train_pd['input'])\n",
    "masked_text = [apply_infilling_masking(x) for x in train_text]\n",
    "textGT = list(train_pd['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과: ['별', '한', '게토', '았깝', '땀', '.', '왜', '싸람', '듯', '릭', '펼', '1캐', '를', '쥰눈', '징', '컥', '꺾', '폰', '싸람', '믐', '롯', '섞', '맒록', '섧멍', '핥', '쟈', '닐', '탯', '끎룐눈', '녀뮤', '퀼', '교', '...', '야뭍', '툰', '둠', '변', '닺씨', '깍', '낄', '싫훈', '굣', '.', '깸삥', '읊', '20', '여', '년', '댜녁', '뵨', '곧', '중', '쩨윌', '귑푼', '낙', '팠', '떤', '곶', '.']\n"
     ]
    }
   ],
   "source": [
    "# 저장된 토크나이저 로드\n",
    "tokenizer = Tokenizer.from_file(\"tokenizers/BPE_tokenizer_50000.json\")\n",
    "text_sample =  '별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 섧멍핥쟈닐 탯끎룐눈 녀뮤 퀼교... 야뭍툰 둠 변 닺씨 깍낄 싫훈 굣. 깸삥읊 20여 년 댜녁뵨 곧 중 쩨윌 귑푼 낙팠떤 곶.'\n",
    "# 테스트\n",
    "input_encoded = tokenizer.encode(text_sample)\n",
    "print(\"토큰화 결과:\", input_encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    inputs ('str' list): Text infilling된 난독화 텍스트 리스트\n",
    "    targets ('str' list): 원본 난독화 텍스트 리스트\n",
    "    tokenizer : 커스텀 토크나이저 (BPE, WordPiece 등)\n",
    "    max_len : 원본 문자열 최대 길이\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs, targets, tokenizer, max_len):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_text = self.inputs[index]\n",
    "        target_text = self.targets[index]\n",
    "\n",
    "        # 입력 텍스트 토큰화 (타겟 텍스트는 별도로 사용 안 함)\n",
    "        input_encoded = self.tokenizer.encode(input_text)\n",
    "        target_encoded = self.tokenizer.encode(target_text)\n",
    "\n",
    "        # 토큰 ID와 패딩 적용\n",
    "        input_ids = input_encoded.ids\n",
    "        target_ids = target_encoded.ids\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # 시퀀스 길이 조정\n",
    "        if len(input_ids) < self.max_len:\n",
    "            # 패딩 추가\n",
    "            pad_length = self.max_len - len(input_ids)\n",
    "            target_pad_length = self.max_len - len(target_ids)\n",
    "            input_ids += [self.tokenizer.token_to_id(\"<pad>\")] * pad_length\n",
    "            target_ids += [self.tokenizer.token_to_id(\"<pad>\")] * target_pad_length\n",
    "            attention_mask += [0] * pad_length\n",
    "        else:\n",
    "            # 길이 초과 시 자르기\n",
    "            input_ids = input_ids[:self.max_len]\n",
    "            target_ids = target_ids[:self.max_len]\n",
    "            attention_mask = attention_mask[:self.max_len]\n",
    "\n",
    "        # 텐서로 변환\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        target_ids = torch.tensor(target_ids, dtype=torch.long)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids.squeeze(0),\n",
    "            \"attention_mask\": attention_mask.squeeze(0),\n",
    "            \"labels\": target_ids.squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  904, 13195,  1685,  ...,     0,     0,     0],\n",
      "        [ 1404,   908,  6260,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([[  904, 13195,  1685,  ...,     0,     0,     0],\n",
      "        [ 1404,   908,  6260,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "dataset = EncoderDataset(masked_text, train_text, tokenizer=tokenizer, max_len=2000)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "batchiter = iter(dataloader)\n",
    "batch = next(batchiter)\n",
    "print(batch['input_ids'])\n",
    "print(batch['attention_mask'])\n",
    "print(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1031\n",
    "config = BartConfig(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    d_model=512,\n",
    "    encoder_layers=3,\n",
    "    encoder_attention_heads=4,\n",
    "    max_position_embeddings=2000\n",
    ")\n",
    "model = BartModel(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2000])\n",
      "torch.Size([2, 2000])\n",
      "torch.Size([2, 2000])\n",
      "torch.Size([2, 2000, 512])\n",
      "torch.Size([2, 2000, 50000])\n",
      "torch.Size([4000, 50000])\n",
      "torch.Size([4000])\n"
     ]
    }
   ],
   "source": [
    "batchiter = iter(dataloader)\n",
    "batch = next(batchiter)\n",
    "\n",
    "model.to(device)\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# 선형 레이어 추가\n",
    "vocab_size = tokenizer.get_vocab_size()  # 예시 vocab_size\n",
    "linear_layer = nn.Linear(512, vocab_size).to(device)\n",
    "\n",
    "# 모델 출력 후 변환\n",
    "outputs = model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "logits = outputs.last_hidden_state  # torch.Size([2, 2000, 512])\n",
    "print(logits.shape)\n",
    "\n",
    "# 선형 변환을 통해 logits 크기 조정\n",
    "logits = linear_layer(logits)  # torch.Size([2, 2000, vocab_size])\n",
    "print(logits.shape)\n",
    "\n",
    "# 손실 계산을 위한 형태 변환\n",
    "logits = logits.view(-1, vocab_size)  # torch.Size([4000, vocab_size])\n",
    "print(logits.shape)\n",
    "labels = labels.view(-1)              # torch.Size([4000])\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textEncoder(nn.Module):\n",
    "    def __init__(self, config, vocab_size):\n",
    "        super(textEncoder, self).__init__()\n",
    "        self.bart = BartModel(config)\n",
    "        # d_model = 512로 가정. 트랜스포머 논문의 ffn층과 똑같이 설정.\n",
    "        # torch.Size([batch_size, seq_len, config.d_model]) -> torch.Size([batch_size*seq_len, 4*config.d_model])\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_model*4)\n",
    "        self.relu = nn.ReLU()\n",
    "        # torch.Size([batch_size*seq_len, 4*config.d_model]) -> torch.Size([batch_size*seq_len, vocab_size])\n",
    "        self.fc2 = nn.Linear(config.d_model*4, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bart.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = self.fc1(outputs.last_hidden_state)  # torch.Size([batch_size, seq_len, vocab_size])\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        logits = x.view(-1, x.size(-1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2000])\n",
      "torch.Size([2, 2000])\n",
      "torch.Size([2, 2000])\n"
     ]
    }
   ],
   "source": [
    "encoder = textEncoder(config=config, vocab_size=tokenizer.get_vocab_size())\n",
    "batch = next(batchiter)\n",
    "\n",
    "encoder.to(device)\n",
    "input_ids = batch['input_ids'].to(device)\n",
    "attention_mask = batch['attention_mask'].to(device)\n",
    "labels = batch['labels'].to(device)\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1824, -0.3125, -0.2508,  ...,  0.1905,  0.2474, -0.0953],\n",
       "        [-0.3635, -0.1750, -0.3664,  ...,  0.2590,  0.1992,  0.0099],\n",
       "        [ 0.1408,  0.1292,  0.6883,  ..., -0.1493,  0.2939,  0.1631],\n",
       "        ...,\n",
       "        [-0.2539, -0.2399, -0.1096,  ..., -0.1341,  0.0103, -0.3312],\n",
       "        [-0.3597,  0.0321, -0.2552,  ..., -0.0444,  0.1883, -0.4349],\n",
       "        [ 0.1146, -0.0955, -0.0164,  ...,  0.0586,  0.3599, -0.5241]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = encoder(input_ids, attention_mask)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.view(-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4000, 50000]), torch.Size([4000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1359,  383, 1688, 1234, 1695], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5631 [00:00<50:48,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 3\n",
      "Generator loss: 11.08111763, Discriminator loss: 11.08111763\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/5631 [00:01<48:58,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 10.40688133, Discriminator loss: 10.40688133\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/5631 [00:01<48:29,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 9.79317856, Discriminator loss: 9.79317856\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/5631 [00:02<48:16,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 9.22988129, Discriminator loss: 9.22988129\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/5631 [00:02<48:08,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 8.71735764, Discriminator loss: 8.71735764\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/5631 [00:03<48:03,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 8.24635983, Discriminator loss: 8.24635983\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/5631 [00:03<47:58,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 7.80170345, Discriminator loss: 7.80170345\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/5631 [00:04<47:58,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 7.38872147, Discriminator loss: 7.38872147\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/5631 [00:04<47:58,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 6.98641443, Discriminator loss: 6.98641443\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/5631 [00:05<47:56,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 6.59589052, Discriminator loss: 6.59589052\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/5631 [00:05<47:54,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 6.20864964, Discriminator loss: 6.20864964\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/5631 [00:06<47:55,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 5.82960176, Discriminator loss: 5.82960176\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/5631 [00:06<47:51,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 5.45355511, Discriminator loss: 5.45355511\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/5631 [00:07<47:54,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 5.08133459, Discriminator loss: 5.08133459\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/5631 [00:07<47:54,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 4.71161842, Discriminator loss: 4.71161842\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/5631 [00:08<47:53,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 4.34750319, Discriminator loss: 4.34750319\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/5631 [00:08<47:51,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 3.98488474, Discriminator loss: 3.98488474\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/5631 [00:09<48:04,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 3.62567234, Discriminator loss: 3.62567234\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/5631 [00:09<48:05,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loss: 3.27260447, Discriminator loss: 3.27260447\n",
      "Epoch 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/5631 [00:10<50:33,  1.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerator loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Discriminator loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py:1052\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m-> 1052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 2\n",
    "optimizer = AdamW(encoder.parameters(), lr=5e-5)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    for idx, data in tqdm(enumerate(dataloader), total=int(len(dataset)/batch_size)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        labels = labels.view(-1)  \n",
    "\n",
    "        predicted_labels = encoder(input_ids, attention_mask)\n",
    "        loss = loss_func(predicted_labels, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} of {epochs}\")\n",
    "    print(f\"Generator loss: {loss:.8f}, Discriminator loss: {loss:.8f}\")\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
