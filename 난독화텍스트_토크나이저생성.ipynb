{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'datas/test.csv'\n",
    "train_path = 'datas/train.csv'\n",
    "\n",
    "test_pd = pd.read_csv(test_path)\n",
    "train_pd = pd.read_csv(train_path)\n",
    "train_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2fix = train_pd['input']\n",
    "textGT = train_pd['output']\n",
    "\n",
    "max_text2fix_length = max([len(x) for x in text2fix])\n",
    "max_textGT_length = max([len(x) for x in textGT])\n",
    "\n",
    "print(max_text2fix_length, max_textGT_length)\n",
    "# 두 문장 모두 최대길이는 1381 1381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_test_length = max([len(x) for x in test_pd['input']])\n",
    "max_test_length, len(test_pd['input'])\n",
    "# 가장 긴 생성할 테스트 난독화 텍스트 길이 : 1965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2reconstruct_string = ''\n",
    "for strings in text2fix:\n",
    "    text2reconstruct_string = text2reconstruct_string + strings + '\\n'\n",
    "text2reconstruct_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strings in test_pd['input']:\n",
    "    text2reconstruct_string = text2reconstruct_string + strings + '\\n'\n",
    "text2reconstruct_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 파일로 저장\n",
    "file_path = 'datas/text2reconstruct.txt'\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(text2reconstruct_string)\n",
    "\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# # 1. WordPiece 모델 생성\n",
    "# tokenizer = Tokenizer(models.WordPiece(unk_token=\"<unk>\"))\n",
    "\n",
    "# # 2. 텍스트 정규화 및 전처리 설정\n",
    "# tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC()])  # 유니코드 정규화\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()  # 공백 단위 분리\n",
    "\n",
    "# # 3. 학습 설정\n",
    "# trainer = trainers.WordPieceTrainer(\n",
    "#     vocab_size=50000,  # 어휘 크기 조정 가능\n",
    "#     special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<mask>\"]\n",
    "# )\n",
    "\n",
    "# # 4. 텍스트 파일 학습 (파일 경로 정확히 설정 필요)\n",
    "# files = [\"datas/text2reconstruct.txt\"]\n",
    "# tokenizer.train(files, trainer)\n",
    "\n",
    "# # 5. 후처리: 문장 앞뒤에 <bos>, <eos> 토큰 추가\n",
    "# tokenizer.post_processor = TemplateProcessing(\n",
    "#     single=\"<bos> $A <eos>\",\n",
    "#     pair=\"<bos> $A <eos> <bos> $B:1 <eos>:1\",\n",
    "#     special_tokens=[(\"<bos>\", 1), (\"<eos>\", 2)]\n",
    "# )\n",
    "\n",
    "# # 6. 학습된 토크나이저 저장 및 테스트\n",
    "# tokenizer.save(\"wordpiece_tokenizer_50000.json\")\n",
    "\n",
    "# # 토큰화 테스트\n",
    "# for s in text2fix[:5]:\n",
    "#     print(\"원본텍스트:\", s)\n",
    "#     encoded = tokenizer.encode(s)\n",
    "#     print(\"토큰화 결과:\", encoded.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "# 토크나이저와 BPE 모델 생성\n",
    "BPE_tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# BPE 트레이너 설정\n",
    "trainer = trainers.BpeTrainer(special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\", \"<mask>\"], vocab_size=50000)\n",
    "BPE_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# 텍스트 파일 학습\n",
    "files = [\"datas/encoder_augmentation.txt\"]  # 정확한 경로로 수정 필요\n",
    "BPE_tokenizer.train(files, trainer)\n",
    "\n",
    "# 토큰화 테스트\n",
    "for s in text2fix[:5]:\n",
    "    print(\"원본텍스트:\", s)\n",
    "    encoded = BPE_tokenizer.encode(s)\n",
    "    print(\"토큰화 결과:\", encoded.tokens)\n",
    "\n",
    "# 학습된 토크나이저 저장 및 테스트\n",
    "BPE_tokenizer.save(\"BPE_tokenizer_50000_aug.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 지정\n",
    "file_path = 'datas/text2reconstruct.txt'\n",
    "\n",
    "# 한 줄씩 읽어서 리스트로 저장\n",
    "lines = []\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()  # 앞뒤 공백 제거\n",
    "        if line:             # 빈 줄은 제외\n",
    "            lines.append(line)\n",
    "\n",
    "# 읽어온 결과 출력\n",
    "print(\"총 {}줄의 텍스트를 읽어왔습니다.\".format(len(lines)))\n",
    "for i, line in enumerate(lines[:5]):  # 첫 5줄 출력\n",
    "    print(f\"{i + 1}: {line}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for line in lines:  # 첫 5줄 출력\n",
    "    encoded = tokenizer.encode(line)\n",
    "    length_encoded = len(encoded.type_ids)\n",
    "    max_length = length_encoded if length_encoded > max_length else max_length\n",
    "print(max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
