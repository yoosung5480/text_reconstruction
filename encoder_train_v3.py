import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.amp as amp
from tokenizers import Tokenizer
from torch.utils.data import DataLoader, Dataset
from transformers import BartConfig, BartModel
from torch.optim import AdamW
from tqdm import tqdm
import random
'''
v2ê¹Œì§€ëŠ” ì¸ì½”ë” í›ˆë ¨ê³¼ì •ì— ì“¸ëŒ€ì—†ëŠ” Æ’cì¸µì´ í¬í•¨ë¼ì„œ ì¸ì½”ë”ì˜ ë²”ì£¼ë¥¼ ë„˜ê¸°ëŠ” í›ˆë ¨ì„ ì‹œì¼°ë‹¤.
ìƒˆë¡œì´ ì •ì˜í•  v3ì½”ë“œì—ì„œëŠ” ì„ë² ë”© ë²¡í„° ìƒì„±í›ˆë ¨ì—ë§Œ ì£¼ëª©í•œë‹¤. 
'''

def apply_infilling_masking(text, mask_token="<mask>", mask_prob=0.15, max_mask_size=3):
    """
    Infilling Maskingì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜.
    
    Args:
        text (str): ì›ë³¸ í…ìŠ¤íŠ¸
        mask_token (str): ë§ˆìŠ¤í‚¹ í† í° (ë””í´íŠ¸: <mask>)
        mask_prob (float): í† í°ì„ ë§ˆìŠ¤í‚¹í•  í™•ë¥  (ë””í´íŠ¸: 0.15)
        max_mask_size (int): ìµœëŒ€ ì—°ì† ë§ˆìŠ¤í‚¹ í† í° ìˆ˜ (ë””í´íŠ¸: 3)

    Returns:
        str: ë§ˆìŠ¤í‚¹ëœ í…ìŠ¤íŠ¸
    """
    tokens = text.split()
    num_masks = max(1, int(len(tokens) * mask_prob))
    mask_positions = random.sample(range(len(tokens)), num_masks)
    for pos in mask_positions:
        mask_length = random.randint(1, max_mask_size)
        tokens[pos:pos + mask_length] = [mask_token]
    return " ".join(tokens)

class EncoderDataset(Dataset):
    """
    inputs ('str' list): Text infillingëœ ë‚œë…í™” í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    targets ('str' list): ì›ë³¸ ë‚œë…í™” í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    tokenizer : ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € (BPE, WordPiece ë“±)
    max_len : ì›ë³¸ ë¬¸ìì—´ ìµœëŒ€ ê¸¸ì´
    """
    def __init__(self, inputs, targets, tokenizer, max_len):
        self.inputs = inputs
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, index):
        input_text = self.inputs[index]
        target_text = self.targets[index]
        input_encoded = self.tokenizer.encode(input_text)
        target_encoded = self.tokenizer.encode(target_text)

        input_ids = input_encoded.ids[:self.max_len]
        target_ids = target_encoded.ids[:self.max_len]
        attention_mask = [1] * len(input_ids)

        pad_id = self.tokenizer.token_to_id("<pad>")
        input_ids += [pad_id] * (self.max_len - len(input_ids))
        target_ids += [pad_id] * (self.max_len - len(target_ids))
        attention_mask += [0] * (self.max_len - len(attention_mask))

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels": torch.tensor(target_ids, dtype=torch.long)
        }

# ë“œë¡­ì•„ì›ƒì„ ì ìš©í•´ë³´ì!
class textEncoder(nn.Module):
    def __init__(self, config):
        super(textEncoder, self).__init__()
        self.bart = BartModel(config)  # ê·¸ëŒ€ë¡œ ìœ ì§€ (FC ë ˆì´ì–´ ì‚­ì œ)

    def forward(self, input_ids, attention_mask):
        outputs = self.bart.encoder(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state  # FC ë ˆì´ì–´ ì—†ì´ ì„ë² ë”© ë²¡í„° ì¶œë ¥


def train(
        train_path='datas/train.csv',
        save_path="trained_encoder.pth",
        tokenizer_path="tokenizers/BPE_tokenizer_50000_aug.json",
        epochs=20,  # ì´ˆê¸° ê°’ ì¦ê°€ (ì¡°ê¸° ì¢…ë£Œ ì ìš©)
        batch_size=8,
        sample_size=500,
        d_model=768,
        encoder_layers=3,
        encoder_attention_heads=4,
        patience=3,  # Early Stopping ê¸°ì¤€ (N ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ)
        min_delta=0.0001  # ì†ì‹¤ ê°ì†Œ ê¸°ì¤€
    ):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    if sample_size == -1:
        train_pd = pd.read_csv(train_path)
    else:
        train_pd = pd.read_csv(train_path).sample(sample_size)

    train_text = list(train_pd['input'])
    masked_text = [apply_infilling_masking(x) for x in train_text]

    tokenizer = Tokenizer.from_file(tokenizer_path)
    dataset = EncoderDataset(masked_text, train_text, tokenizer=tokenizer, max_len=1024)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)

    config = BartConfig(
        vocab_size=tokenizer.get_vocab_size(),
        d_model=d_model,
        encoder_layers=encoder_layers,
        encoder_attention_heads=encoder_attention_heads,
        max_position_embeddings=1024
    )

    encoder = textEncoder(config=config).to(device)
    scaler = amp.GradScaler()
    optimizer = AdamW(encoder.parameters(), lr=5e-5)
    loss_func = nn.MSELoss()

    losses = []
    best_loss = float('inf')  # ìµœì†Œ ì†ì‹¤ ì´ˆê¸°í™”
    patience_counter = 0  # ì¡°ê¸° ì¢…ë£Œ ì¹´ìš´í„°

    for epoch in range(epochs):
        encoder.train()
        total_loss = .0

        for idx, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc=f"Epoch {epoch+1}/{epochs}"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()

            with amp.autocast(device_type=device.type):
                predicted_embeddings = encoder(input_ids, attention_mask)
                target_embeddings = encoder(labels, attention_mask)
                loss = loss_func(predicted_embeddings, target_embeddings)

            if torch.isnan(loss):
                print(f"NaN loss detected at step {idx}, skipping...")
                continue

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        losses.append(avg_loss)

        print(f"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f}")

        # ì¡°ê¸° ì¢…ë£Œ ë¡œì§
        if avg_loss < best_loss - min_delta:  # ì†ì‹¤ì´ ê°œì„ ë˜ì—ˆëŠ”ì§€ í™•ì¸
            best_loss = avg_loss
            patience_counter = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ ì¹´ìš´í„° ì´ˆê¸°í™”
            torch.save(encoder.state_dict(), save_path)  # ê°œì„ ë  ë•Œë§ˆë‹¤ ëª¨ë¸ ì €ì¥
            print(f"ğŸ”¹ Model improved. Saved at {save_path}.")
        else:
            patience_counter += 1
            print(f"âš ï¸ No improvement for {patience_counter}/{patience} epochs.")

        if patience_counter >= patience:
            print("â¹ Early Stopping triggered. Training stopped.")
            break  # ì¡°ê¸° ì¢…ë£Œ

    # ì†ì‹¤ ê·¸ë˜í”„ ì¶œë ¥
    plt.plot(losses, label="Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

    print(f"í›ˆë ¨ëœ ëª¨ë¸ì´ '{save_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return encoder
