import json
import pandas as pd
import matplotlib.pyplot as plt
import random

import torch
import torch.nn as nn
import torch.optim as optim
import torch.amp as amp

from tqdm import tqdm

from torch.utils.data import DataLoader, Dataset
from torch.amp import autocast, GradScaler
from torch.optim import AdamW

from transformers import BartModel
from transformers import BartModel, BartTokenizer
from transformers import BartConfig, BartModel
from transformers import AutoModelForSeq2SeqLM
from transformers import AutoTokenizer, AutoModel

from tokenizers import Tokenizer
import torch
import os

num_workers = os.cpu_count() // 2  # CPU ê°œìˆ˜ì˜ ì ˆë°˜ ì‚¬ìš© (ë³´í†µ ìµœì )
print(f"ì¶”ì²œ num_workers ê°’: {num_workers}")

# í˜„ì¬ í† í¬ë‚˜ì´ì € ë³‘ì—´í™”ì™€, num_workersë¥¼ ì‚¬ìš©í•˜ëŠ” ë³‘ë ¬í™”ê°€ ì¶©ë™ì¼ ì¼ìœ¼í‚´ìœ¼ë¡œ, í† í¬ë‚˜ì´ì € ë³‘ë ¬í™” ê¸°ëŠ¥ì„ ì œê±°í•˜ê³ ì‹œëŸ.
os.environ["TOKENIZERS_PARALLELISM"] = "false"



# ë°ì´í„°ì…‹ ì •ì˜
class DecoderDataset(Dataset):
    def __init__(self, df_path):
        df = pd.read_csv(df_path)
        self.inputs = df["input"].tolist()
        self.outputs = df["output"].tolist()

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        input_text = self.inputs[idx]
        output_text = self.outputs[idx]
        return input_text, output_text

# ë””ì½”ë” ë„¤íŠ¸ì›Œí¬ ì •ì˜
class CrossAttentionDecoder(nn.Module):
    def __init__(self, 
                 kobart_tokenizer_vocab_size, 
                 hidden_dim=768, 
                 num_layers=4, 
                 num_heads=8, 
                 dropout=0.1):
        
        super(CrossAttentionDecoder, self).__init__()
        self.hidden_dim = hidden_dim
        self.kobart_tokenizer_vocab_size = kobart_tokenizer_vocab_size

        # Transformer Decoder Layerì— dropout ì¶”ê°€
        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)

        # Fully Connected Layers
        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 4)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)  # Dropout ì¶”ê°€
        self.fc_out = nn.Linear(hidden_dim * 4, kobart_tokenizer_vocab_size)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, input1, input2, tgt_mask=None):
        """
        input1: ë‚œë…í™”ëœ í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¡í„° (Key, Value) -> (batch_size, seq_len, hidden_dim)
        input2: ë³µì›ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¡í„° (Query) -> (batch_size, seq_len, hidden_dim)
        tgt_mask: (batch_size, seq_len, seq_len) í¬ê¸°ì˜ ë§ˆìŠ¤í¬ í…ì„œ (ì™¸ë¶€ì—ì„œ ì œê³µ)
        """
        decoder_output = self.decoder(input2, input1, tgt_mask=tgt_mask)

        x = self.fc1(decoder_output)
        x = self.relu(x)
        x = self.dropout(x)  # Dropout ì ìš©
        x = self.fc_out(x)

        return self.softmax(x)


# kobart, kobart_tokenizer ê°€ì ¸ì˜¤ëŠ” ì½”ë“œ
def get_kobart_and_tokenizer():
    kobart_tokenizer = AutoTokenizer.from_pretrained("gogamza/kobart-base-v2")
    kobart_model = AutoModelForSeq2SeqLM.from_pretrained("gogamza/kobart-base-v2")
    return kobart_tokenizer, kobart_model


# mybart, mybart_tokenizer ê°€ì ¸ì˜¤ëŠ” ì½”ë“œ
def get_mybart_and_tokenizer(tokenizer_path, model_path, model_config):
    mybart_tokenizer = Tokenizer.from_file(tokenizer_path)
    # BART ëª¨ë¸ ì„¤ì •
    mybart = BartModel(model_config)  # BART ëª¨ë¸ ìƒì„±

    # ì €ì¥ëœ textEncoder ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°
    state_dict = torch.load(model_path, map_location="cpu")
    # BART ëª¨ë¸ì˜ ì¸ì½”ë” ë¶€ë¶„ì—ë§Œ ê°€ì¤‘ì¹˜ ë¡œë“œ
    mybart.encoder.load_state_dict(state_dict, strict=False)

    print("âœ… BART ì¸ì½”ë” ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ!")
    return mybart_tokenizer, mybart



# input1(ë‚œë…í™” í…ìŠ¤íŠ¸) ì„ë² ë”© ë²¡í„° ìƒì„±ìš© í•¨ìˆ˜ (ë°°ì¹˜ ë‹¨ìœ„ ì§€ì›)
def get_encodedKr_emb_vec(input_texts, mybart_tokenizer, mybart_model, device, max_length=1026):
    '''
    mybart_modelëŠ” eval() ëª¨ë“œì—¬ì•¼ í•œë‹¤.
    input_texts: List of input strings (ë°°ì¹˜ ë‹¨ìœ„)
    '''
    input_ids_batch = []
    attention_mask_batch = []

    for input_text in input_texts:
        input_encoded = mybart_tokenizer.encode(input_text)
        input_ids = input_encoded.ids[:max_length]
        attention_mask = [1] * len(input_ids)

        pad_id = mybart_tokenizer.token_to_id("<pad>")
        input_ids += [pad_id] * (max_length - len(input_ids))
        attention_mask += [0] * (max_length - len(attention_mask))

        input_ids_batch.append(input_ids)
        attention_mask_batch.append(attention_mask)

    # (batch_size, seq_len) í˜•íƒœë¡œ ë³€í™˜
    input_ids = torch.tensor(input_ids_batch, dtype=torch.long).to(device)
    attention_mask = torch.tensor(attention_mask_batch, dtype=torch.long).to(device)

    input_emb = mybart_model.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
    return input_emb  # (batch_size, seq_len, hidden_dim)

# input2(í•œêµ­ì–´ í…ìŠ¤íŠ¸) ì„ë² ë”© ë²¡í„° ìƒì„±ìš© í•¨ìˆ˜ (ë°°ì¹˜ ë‹¨ìœ„ ì§€ì›)
def get_korean_emb_vec(output_texts, kobart_tokenizer, kobart_model, device, max_length=1026):
    '''
    kobart_modelëŠ” eval() ëª¨ë“œì—¬ì•¼ í•œë‹¤.
    output_texts: List of output strings (ë°°ì¹˜ ë‹¨ìœ„)
    '''
    output_ids_batch = kobart_tokenizer(output_texts, return_tensors="pt", padding="max_length",
                                        max_length=max_length, truncation=True)["input_ids"].to(device)

    output_emb = kobart_model(output_ids_batch).encoder_last_hidden_state
    return output_emb  # (batch_size, seq_len, hidden_dim)


def reconstruct_text(token_ids, kobart_tokenizer):
    '''
    ì‹œí€€ì…œí•œ í† í°ë²ˆí˜¸ -> ì›ë³µí…ìŠ¤íŠ¸ ë³µêµ¬ì‘ì—….
     predicted_token_ids    (batch_size, max_length) í…ì„œ
     kobart_tokenizer       (í•œêµ­ì–´ í† í¬ë‚˜ì´ì €)
    '''
    decoded_texts = []
    batch_size = token_ids.shape[1]  # ë°°ì¹˜ í¬ê¸°

    for i in range(batch_size):  # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë°˜ë³µ
        token_ids = token_ids[:, i].tolist()  # ğŸ”¥ (max_len,) í˜•íƒœë¡œ ë³€í™˜
        decoded_text = kobart_tokenizer.decode(token_ids)  # ğŸ”¥ ê°œë³„ ë¬¸ì¥ ë³µì›
        decoded_texts.append(decoded_text)    
    for i, text in enumerate(decoded_texts):
        print(f"ğŸ”¹ ë³µì›ëœ ë¬¸ì¥ {i+1}: {text}")


# ----------------------------------------------------------------------------------------------------------------------------------------------------
def decoder_train(
        datset_path='datas/decoder_augmentation.csv',
        tokenizer_path="tokenizers/BPE_tokenizer_50000_aug.json",
        model_path="trained_encoder3.pth",
        model_save_path="trained_decoder.pth",
        batch_size=2,
        epochs=3,
        batch_max_len=1026,
        learning_rate=5e-5,
        patience=2,  # ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€ (ì—°ì† `patience` íšŸìˆ˜ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ)
        min_delta=0.001,  # ê°œì„  ìµœì†Œ ê¸°ì¤€ (ì´í•˜ë¡œ ê°œì„ ë˜ë©´ ì¢…ë£Œ)
        
        model_config=BartConfig(
            vocab_size=50000,
            d_model=768,
            encoder_layers=4,
            encoder_attention_heads=8,
            max_position_embeddings=1026,
        ),

        mydecoder=CrossAttentionDecoder(
            kobart_tokenizer_vocab_size=30000,
            hidden_dim=768,
            num_layers=1,
            num_heads=1,
            dropout=0.1
        ),
):
    # ë°ì´í„°ì…‹, ë°ì´í„°ë¡œë” ì •ì˜
    decoder_dataset = DecoderDataset(df_path=datset_path)
    num_workers = os.cpu_count() // 2  # âœ… ì‹œìŠ¤í…œì— ë§ëŠ” ìµœì ì˜ CPU ì½”ì–´ ì‚¬ìš©
    dataloader = DataLoader(
        decoder_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
    )

    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    kobart_tokenizer, kobart_model = get_kobart_and_tokenizer()
    mybart_tokenizer, mybart_model = get_mybart_and_tokenizer(tokenizer_path, model_path, model_config)

    # ëª¨ë¸ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    kobart_model.eval(), mybart_model.eval()
    kobart_model.to(device), mybart_model.to(device)
    mydecoder.to(device)

    # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
    loss_fn = nn.CrossEntropyLoss(ignore_index=kobart_tokenizer.pad_token_id)
    optimizer = optim.AdamW(mydecoder.parameters(), lr=learning_rate)
    scaler = GradScaler()

    # í•™ìŠµ ê¸°ë¡ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    loss_history = []
    accuracy_history = []

    # Early Stopping ì„¤ì •
    best_loss = float('inf')
    epochs_no_improve = 0

    # í•™ìŠµ ë£¨í”„ ì‹œì‘
    for epoch in range(epochs):
        mydecoder.train()
        total_loss = 0
        total_correct = 0
        total_tokens = 0

        tqdm_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f"Epoch {epoch + 1}/{epochs}")

        for step, (inputs, outputs) in tqdm_bar:
            optimizer.zero_grad()

            # ğŸ”¹ ë°°ì¹˜ ë‚´ ìµœëŒ€ `seq_len` ì°¾ê¸°
            input_emb = get_encodedKr_emb_vec(inputs, mybart_tokenizer, mybart_model, device).permute(1, 0, 2)
            output_emb = get_korean_emb_vec(outputs, kobart_tokenizer, kobart_model, device).permute(1, 0, 2)

            # ğŸ”¹ `tgt_mask` ì°¨ì› í™•ì¥
            tgt_mask = nn.Transformer.generate_square_subsequent_mask(batch_max_len).to(device)

            # ğŸ”¹ Mixed Precision Training ì ìš©
            with autocast(device_type=device.type):
                decoder_output = mydecoder(input_emb, output_emb, tgt_mask=tgt_mask)

                # ğŸ”¹ ì •ë‹µ ë°ì´í„° ë¡œë“œ
                ground_truth_token_ids = kobart_tokenizer(outputs, return_tensors="pt", padding="max_length",
                                                          max_length=batch_max_len, truncation=True)["input_ids"].T.to(device)

                # ğŸ”¹ ì†ì‹¤ ê³„ì‚°
                loss = loss_fn(decoder_output.reshape(-1, decoder_output.size(-1)), ground_truth_token_ids.reshape(-1))

            # ğŸ”¹ Backpropagation
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # ğŸ”¹ ì†ì‹¤ ë° ì •ë‹µë¥  ê³„ì‚°
            total_loss += loss.item()

            # ğŸ”¹ ì˜ˆì¸¡ê°’ & ì •ë‹µ ë¹„êµ (íŒ¨ë”© ì œì™¸)
            predicted_token_ids = decoder_output.argmax(dim=-1)  # (seq_len, batch_size)
            correct = ((predicted_token_ids == ground_truth_token_ids) & (ground_truth_token_ids != kobart_tokenizer.pad_token_id)).sum().item()
            valid_tokens = (ground_truth_token_ids != kobart_tokenizer.pad_token_id).sum().item()

            total_correct += correct
            total_tokens += valid_tokens

            accuracy = correct / valid_tokens if valid_tokens > 0 else 0

            tqdm_bar.set_postfix(loss=loss.item(), accuracy=accuracy)

        # ğŸ”¹ Early Stopping ì²´í¬
        epoch_loss = total_loss / len(dataloader)
        epoch_accuracy = total_correct / total_tokens if total_tokens > 0 else 0

        if epoch_loss < best_loss - min_delta:
            best_loss = epoch_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= patience:
            print(f"\nâ¹ï¸ Early stopping triggered! No improvement for {patience} consecutive epochs.")
            break

    print("\nğŸ‰ í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ê³¼ í•™ìŠµ ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
