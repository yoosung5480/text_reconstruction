{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.amp as amp\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import BartModel\n",
    "from transformers import BartModel, BartTokenizer\n",
    "from transformers import BartConfig, BartModel\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추천 num_workers 값: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "num_workers = os.cpu_count() // 2  # CPU 개수의 절반 사용 (보통 최적)\n",
    "print(f\"추천 num_workers 값: {num_workers}\")\n",
    "\n",
    "# 현재 토크나이저 병열화와, num_workers를 사용하는 병렬화가 충동일 일으킴으로, 토크나이저 병렬화 기능을 제거하고시돟.\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전학습된 koBart, train_encoder 가져오기.\n",
    "\n",
    "샘플 텍스트를 각각 1줄씩 넣어보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의\n",
    "class DecoderDataset(Dataset):\n",
    "    def __init__(self, df_path):\n",
    "        df = pd.read_csv(df_path)\n",
    "        self.inputs = df[\"input\"].tolist()\n",
    "        self.outputs = df[\"output\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        output_text = self.outputs[idx]\n",
    "        return input_text, output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kobart, kobart_tokenizer 가져오는 코드\n",
    "def get_kobart_and_tokenizer():\n",
    "    kobart_tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "    kobart_model = AutoModelForSeq2SeqLM.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "    return kobart_tokenizer, kobart_model\n",
    "\n",
    "\n",
    "# mybart, mybart_tokenizer 가져오는 코드\n",
    "def get_mybart_and_tokenizer(tokenizer_path, model_path, model_config):\n",
    "    mybart_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "    # BART 모델 설정\n",
    "    mybart = BartModel(model_config)  # BART 모델 생성\n",
    "\n",
    "    # 저장된 textEncoder 모델 가중치 불러오기\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "    # BART 모델의 인코더 부분에만 가중치 로드\n",
    "    mybart.encoder.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    print(\"✅ BART 인코더 가중치 로드 완료!\")\n",
    "    return mybart_tokenizer, mybart\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1(난독화 텍스트) 임베딩 벡터 생성용 함수 (배치 단위 지원)\n",
    "def get_encodedKr_emb_vec(input_texts, mybart_tokenizer, mybart_model, device, max_length=1026):\n",
    "    '''\n",
    "    mybart_model는 eval() 모드여야 한다.\n",
    "    input_texts: List of input strings (배치 단위)\n",
    "    '''\n",
    "    input_ids_batch = []\n",
    "    attention_mask_batch = []\n",
    "\n",
    "    for input_text in input_texts:\n",
    "        input_encoded = mybart_tokenizer.encode(input_text)\n",
    "        input_ids = input_encoded.ids[:max_length]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        pad_id = mybart_tokenizer.token_to_id(\"<pad>\")\n",
    "        input_ids += [pad_id] * (max_length - len(input_ids))\n",
    "        attention_mask += [0] * (max_length - len(attention_mask))\n",
    "\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "\n",
    "    # (batch_size, seq_len) 형태로 변환\n",
    "    input_ids = torch.tensor(input_ids_batch, dtype=torch.long).to(device)\n",
    "    attention_mask = torch.tensor(attention_mask_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    input_emb = mybart_model.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "    return input_emb  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "# input2(한국어 텍스트) 임베딩 벡터 생성용 함수 (배치 단위 지원)\n",
    "def get_korean_emb_vec(output_texts, kobart_tokenizer, kobart_model, device, max_length=1026):\n",
    "    '''\n",
    "    kobart_model는 eval() 모드여야 한다.\n",
    "    output_texts: List of output strings (배치 단위)\n",
    "    '''\n",
    "    output_ids_batch = kobart_tokenizer(output_texts, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                        max_length=max_length, truncation=True)[\"input_ids\"].to(device)\n",
    "\n",
    "    output_emb = kobart_model(output_ids_batch).encoder_last_hidden_state\n",
    "    return output_emb  # (batch_size, seq_len, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 디코더의 네트워크 구조 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 kobart_tokenizer_vocab_size, \n",
    "                 hidden_dim=768, \n",
    "                 num_layers=6, \n",
    "                 num_heads=8, \n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super(CrossAttentionDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kobart_tokenizer_vocab_size = kobart_tokenizer_vocab_size\n",
    "\n",
    "        # Transformer Decoder Layer에 dropout 추가\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout 추가\n",
    "        self.fc_out = nn.Linear(hidden_dim * 4, kobart_tokenizer_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input1, input2, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        input1: 난독화된 텍스트 임베딩 벡터 (Key, Value) -> (batch_size, seq_len, hidden_dim)\n",
    "        input2: 복원된 한국어 텍스트 임베딩 벡터 (Query) -> (batch_size, seq_len, hidden_dim)\n",
    "        tgt_mask: (batch_size, seq_len, seq_len) 크기의 마스크 텐서 (외부에서 제공)\n",
    "        \"\"\"\n",
    "        decoder_output = self.decoder(input2, input1, tgt_mask=tgt_mask)\n",
    "\n",
    "        x = self.fc1(decoder_output)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # Dropout 적용\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return self.softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 에포크 작성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 일단 1개의 배치만 현재 네트워크에 넣어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋, 데이터로더 정의\n",
    "datset_path = 'datas/decoder_augmentation.csv'\n",
    "decoder_dataset = DecoderDataset(df_path= datset_path)\n",
    "dataloader = DataLoader(\n",
    "    decoder_dataset, \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "dataiter = iter(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/tmp/ipykernel_150021/1922565671.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BART 인코더 가중치 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "# 사전학습된 토크나이저, BART인코더 가져오기\n",
    "\n",
    "# 저장해놓은 모델의 네트워크 구조와 동일해야한다.\n",
    "model_config = BartConfig(\n",
    "    vocab_size=50000,\n",
    "    d_model=768,\n",
    "    encoder_layers=4,\n",
    "    encoder_attention_heads=8,\n",
    "    max_position_embeddings=1026,\n",
    ")\n",
    "tokenizer_path = \"tokenizers/BPE_tokenizer_50000_aug.json\"\n",
    "model_path = \"trained_encoder3.pth\"\n",
    "\n",
    "# 토크나이저, 모델을 불러오는 코드.\n",
    "kobart_tokenizer, kobart_model = get_kobart_and_tokenizer()\n",
    "mybart_tokenizer, mybart_model = get_mybart_and_tokenizer(tokenizer_path, model_path, model_config)\n",
    "\n",
    "# kobart_tokenizer.max_len_single_sentence, kobart_tokenizer.vocab_size # (1000000000000000019884624838656, 30000)\n",
    "# kobart_model.config.d_model, kobart_model.config.max_position_embeddings # (768, 1026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BartForConditionalGeneration(\n",
       "   (model): BartModel(\n",
       "     (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "     (encoder): BartEncoder(\n",
       "       (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "       (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "       (layers): ModuleList(\n",
       "         (0-5): 6 x BartEncoderLayer(\n",
       "           (self_attn): BartSdpaAttention(\n",
       "             (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (decoder): BartDecoder(\n",
       "       (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "       (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "       (layers): ModuleList(\n",
       "         (0-5): 6 x BartDecoderLayer(\n",
       "           (self_attn): BartSdpaAttention(\n",
       "             (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartSdpaAttention(\n",
       "             (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       " ),\n",
       " BartModel(\n",
       "   (shared): BartScaledWordEmbedding(50000, 768, padding_idx=1)\n",
       "   (encoder): BartEncoder(\n",
       "     (embed_tokens): BartScaledWordEmbedding(50000, 768, padding_idx=1)\n",
       "     (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "     (layers): ModuleList(\n",
       "       (0-3): 4 x BartEncoderLayer(\n",
       "         (self_attn): BartSdpaAttention(\n",
       "           (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         )\n",
       "         (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (activation_fn): GELUActivation()\n",
       "         (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "         (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "         (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "     )\n",
       "     (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (decoder): BartDecoder(\n",
       "     (embed_tokens): BartScaledWordEmbedding(50000, 768, padding_idx=1)\n",
       "     (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "     (layers): ModuleList(\n",
       "       (0-11): 12 x BartDecoderLayer(\n",
       "         (self_attn): BartSdpaAttention(\n",
       "           (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         )\n",
       "         (activation_fn): GELUActivation()\n",
       "         (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (encoder_attn): BartSdpaAttention(\n",
       "           (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         )\n",
       "         (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "         (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "         (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "     )\n",
       "     (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델들의 모드 설정 및 device 설정.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 사전학습된 모델들은 훈련파라미터로 사용 x\n",
    "kobart_model.eval(), mybart_model.eval()\n",
    "kobart_model.to(device), mybart_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossAttentionDecoder(\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc_out): Linear(in_features=3072, out_features=30000, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더 정의.\n",
    "mydecoder = CrossAttentionDecoder(\n",
    "    kobart_tokenizer_vocab_size=kobart_tokenizer.vocab_size,\n",
    "    hidden_dim=768,\n",
    "    num_layers=1,\n",
    "    num_heads=1,\n",
    "    dropout=0.1)\n",
    "\n",
    "mydecoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1026, 768]), torch.Size([2, 1026, 768]), torch.Size([1026, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시용 출력임으로, detach해서 훈련에 영향주지 않도록한다.\n",
    "batch = next(dataiter)\n",
    "input_texts , output_texts = batch\n",
    "\n",
    "sample_output_emb = get_korean_emb_vec(output_texts, kobart_tokenizer, kobart_model, device).detach()\n",
    "sample_input_emb = get_encodedKr_emb_vec(input_texts, mybart_tokenizer, mybart_model, device).detach()\n",
    "\n",
    "\n",
    "ground_truth_token_ids = kobart_tokenizer(output_texts, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                          max_length=1026, truncation=True)[\"input_ids\"].T.detach()\n",
    "\n",
    "\n",
    "sample_output_emb.shape, sample_input_emb.shape, ground_truth_token_ids.shape # (torch.Size([1, 1026, 768]), torch.Size([1, 1026, 768]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1026, 2, 768]) torch.Size([1026, 2, 768]) torch.Size([1026, 1026])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1026, 2, 30000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 의 결과 출력.\n",
    "batch_size = 2\n",
    "batch_max_len = 1026\n",
    "for inputs, outputs in dataloader:\n",
    "    # 🔥 배치 내 최대 `seq_len`을 찾기\n",
    "    input_emb = get_encodedKr_emb_vec(input_texts=inputs, \n",
    "                                      mybart_tokenizer=mybart_tokenizer,\n",
    "                                      mybart_model=mybart_model,\n",
    "                                      device=device).permute(1, 0, 2)\n",
    "    output_emb = get_korean_emb_vec(output_texts=outputs,\n",
    "                                    kobart_tokenizer=kobart_tokenizer,\n",
    "                                    kobart_model=kobart_model,\n",
    "                                    device=device).permute(1, 0, 2)\n",
    "    # 🔥 `tgt_mask`를 batch_max_len 기준으로 생성\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(batch_max_len).to(device)\n",
    "\n",
    "    print(input_emb.shape, output_emb.shape, tgt_mask.shape)\n",
    "    output = mydecoder(input_emb, output_emb, tgt_mask=tgt_mask).detach()\n",
    "    break\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = output.reshape(-1, output.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2052, 30000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1026, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([658, 658], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🔹 `output`에서 가장 높은 확률을 가진 토큰 인덱스 가져오기\n",
    "predicted_token_ids = output_emb.argmax(dim=-1)  # (1026, 2)\n",
    "print(predicted_token_ids.shape)\n",
    "predicted_token_ids[1025]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 복원된 문장과 정답 비교하여 오차계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토큰 정확도: 0.0000 (0/2052)\n"
     ]
    }
   ],
   "source": [
    "# 🔹 정답 토큰 변환 (batch_size 단위로 `outputs`을 토큰화)\n",
    "ground_truth_token_ids = kobart_tokenizer(outputs, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                         max_length=batch_max_len, truncation=True)[\"input_ids\"].T.to(device)\n",
    "# 🔹 `.T`를 사용하여 `batch_size`를 두 번째 차원으로 맞춤 (max_len, batch_size)\n",
    "\n",
    "# 🔹 예측값과 정답값 비교\n",
    "correct = (predicted_token_ids == ground_truth_token_ids).sum().item()  # 맞춘 토큰 개수\n",
    "total = batch_max_len * batch_size  # 전체 토큰 개수\n",
    "\n",
    "# 🔹 정확도 계산\n",
    "accuracy = correct / total\n",
    "print(f\"✅ 토큰 정확도: {accuracy:.4f} ({correct}/{total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과를 한국어로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 복원된 문장 1: Pγγγγγγ<unused50>γP<unused50><unused50><unused50><unused50><unused50>PPPPPγPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγ<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPγγγγγγγγγγγγγγγγγγγγγγγγγ<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP1ʈʈʈʈ11ʈʈɤɤɤ11ʈʈʈ111ʈɤɤɤɤɤʈɤɤ111ʈʈɤ111ʈʈɤư1ʈ<sys><sys><sys>111<sys><sys>ưưưưưưưư1<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>χχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχɤɤɤɤɤɤɤɤɤɤʈʈʈʈɤɤʈʈʈɤɤɤɤɤγγγγγγγγγγγγχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγχχχχχχγγγγγχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχγχχγχγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγ\n",
      "🔹 복원된 문장 2: PγγγγPPPPPγPP<unused50><unused50>γ<unused50>PP<unused50>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγ<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPγγγγγγγγγγγγγγγγγγγγγγ<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP1P>;->Ν111ưưưưư1ưư<sys>1111ưɤ111ưưɤ111>;->Х11ưưưưưưưưưư11ưưưưưưưưư<unused50>ư<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>χχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχŠŠʈʈʈɤʈʈʈʈʈʈʈʈʈʈʈʈʈʈʈʈʈʈʈγγγγγγγγγγγγγγγχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγ<unused50><unused50><unused50><unused50><unused50><unused50>γγγγγχχχχχχχχχγχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχχγγχχγχγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγγ\n"
     ]
    }
   ],
   "source": [
    "# 🔹 배치 단위로 복원\n",
    "decoded_texts = []\n",
    "batch_size = predicted_token_ids.shape[1]  # 배치 크기\n",
    "\n",
    "for i in range(batch_size):  # 배치 크기만큼 반복\n",
    "    token_ids = predicted_token_ids[:, i].tolist()  # 🔥 (max_len,) 형태로 변환\n",
    "    decoded_text = kobart_tokenizer.decode(token_ids)  # 🔥 개별 문장 복원\n",
    "    decoded_texts.append(decoded_text)\n",
    "\n",
    "# 🔹 결과 확인\n",
    "for i, text in enumerate(decoded_texts):\n",
    "    print(f\"🔹 복원된 문장 {i+1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 실제문장 문장 1: 매트리스와 베개에서 엄청난 곰팡이 냄새가 납니다. 또 다른 베개에서는 오래된 머리카락 냄새가 납니다. 침구 관리가 전혀 안 하는 듯합니다. 돈이 아깝습니다.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "🔹 실제문장 문장 2: 친절하지 않은 서비스는 없었음. 최소 0의 서비스를 받았고, 최대는 한도가 끝도 없었으나, 이것저것 알고 서칭해서 가야 서비스를 받을 수 있다는 점과 할인 없이 갈 만한 가격대인지는..? 주변에 이것저것 많아서 체력을 고려한 숙소로 정하기 좋았음.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# 🔹 배치 단위로 복원\n",
    "decoded_texts = []\n",
    "batch_size = ground_truth_token_ids.shape[1]  # 배치 크기\n",
    "\n",
    "for i in range(batch_size):  # 배치 크기만큼 반복\n",
    "    token_ids = ground_truth_token_ids[:, i].tolist()  # 🔥 (max_len,) 형태로 변환\n",
    "    decoded_text = kobart_tokenizer.decode(token_ids) # 🔥 개별 문장 복원\n",
    "    decoded_texts.append(decoded_text)\n",
    "\n",
    "# 🔹 결과 확인\n",
    "for i, text in enumerate(decoded_texts):\n",
    "    print(f\"🔹 실제문장 문장 {i+1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 100/16895 [00:50<2:21:13,  1.98it/s, accuracy=0, loss=0.0666]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Epoch 1/1 - Loss: 0.0079 | Accuracy: 0.0000\n",
      "\n",
      "🎉 훈련 완료! 모델과 학습 기록이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 학습 설정\n",
    "epochs = 1\n",
    "batch_size = 2\n",
    "batch_max_len = 1026\n",
    "learning_rate = 5e-5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(mydecoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Mixed Precision Training 설정\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 학습 기록 저장용 리스트\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "# 학습 루프 시작\n",
    "for epoch in range(epochs):\n",
    "    mydecoder.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    tqdm_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for step, (inputs, outputs) in tqdm_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 🔹 배치 내 최대 `seq_len` 찾기\n",
    "        input_emb = get_encodedKr_emb_vec(inputs, mybart_tokenizer, mybart_model, device).permute(1, 0, 2)  # (torch.Size([2, 1026, 768])\n",
    "        output_emb = get_korean_emb_vec(outputs, kobart_tokenizer, kobart_model, device).permute(1, 0, 2)   # (torch.Size([2, 1026, 768])\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(batch_max_len).to(device)                 # torch.Size([1026, 1026])\n",
    "\n",
    "        # 🔹 Mixed Precision Training 적용\n",
    "        with autocast(device_type=device.type):\n",
    "            decoder_output = mydecoder(input_emb, output_emb, tgt_mask=tgt_mask)  # decoder_output: (seq_len, batch_size, vocab_size)\n",
    "            \n",
    "            ground_truth_token_ids = kobart_tokenizer(outputs, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                                    max_length=batch_max_len, truncation=True)[\"input_ids\"].T.to(device)  # (seq_len, batch_size)\n",
    "            \n",
    "\n",
    "            # input1: (batch_size*max_len, vocab_size) 예측한 토큰 로짓값, input2: (batch_size*max_len) gt토큰 id번호\n",
    "            loss = loss_fn(decoder_output.reshape(-1, decoder_output.size(-1)), ground_truth_token_ids.reshape(-1))\n",
    "\n",
    "        \n",
    "        # 🔹 Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # 🔹 손실 및 정답률 계산\n",
    "        total_loss += loss.item()\n",
    "        correct = (predicted_token_ids == ground_truth_token_ids).sum().item()  # 맞춘 토큰 개수\n",
    "        total_correct += correct\n",
    "        total_tokens += batch_max_len * batch_size\n",
    "\n",
    "        accuracy = correct / (batch_max_len * batch_size)\n",
    "\n",
    "        # 🔹 tqdm 업데이트\n",
    "        tqdm_bar.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
    "\n",
    "        if step == 100:\n",
    "            break\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # 🔹 에포크당 평균 손실 및 정확도 기록\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_accuracy = total_correct / total_tokens\n",
    "\n",
    "    loss_history.append(epoch_loss)\n",
    "    accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "    print(f\"\\n✅ Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} | Accuracy: {epoch_accuracy:.4f}\")\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 🔥 훈련 완료 후 손실 및 정답률 저장\n",
    "torch.save(mydecoder, \"trained_decoder_full.pth\")\n",
    "\n",
    "print(\"\\n🎉 훈련 완료! 모델과 학습 기록이 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_150021/2090858286.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_decoder = torch.load(\"trained_decoder_full.pth\")\n",
      "복구 진행 중:  50%|█████     | 1/2 [00:54<00:54, 54.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🛠️ [디버깅] 원본: 껙쉴엣써 본윈눈 부는 청먈 쬐꼰. 홅뗄림 옮레퇴댜 폰닉꺄 젼췌척읕료 뇨휴확돼교 싶섦일 옜껑씨란는 첨운 앝쉽징많, 콴뤽눈 맵욹 청걺학꼐 짧 툇얽 잇타. 쥑건툴읫 읗뎅냐 셔빛슨눈 뮤쩍 췬쩔햐댜. 뭄었뽀닫토 맘뭬 들엇뎐 꼇순 룸섶빚수 뭬뉼. 행묾람먼 갛출. 쥑굼카쥐 머컸턴 랑면 츙예 위툐록 곧굽찐 람먼눈 쩌욺 멱억뵨 듣. 젼폭, 초껙관찬, 쌔유, 꽃께 둥윌 씬션한 헤샨묾위 쩡먈 멱귀 좋궤 랴먼과 얼웁려쪄 냘욘는뗑, 륨엘석 앝씩꿀료 쉭껴 멱깁 탁윅댜. 삐잘, 찍퀸, 깖뛰됴 얗이 뿌쥠학꼰 맡됴 좋댜. 롭삣쯩의 옳큄술랴는 야이륏쉿 폅엡섶 블런췰롤 군냘의 쒜뿌 츄천 쎄뚝 뭬눗량 팬께윗쿠를 식커 먹였눈테, 뚜 걔 댜 캉쮸! 수톄익쿤는 육질곽 꾸웠넨는 청토됴 딱 좋앗교, 뺀꼐잊쿠는 난쌩쪄움 폰눈 삐주열료 풍췸갤철렴 냐욘는 엮누 폿똥의 펜켕익끙와눈 짜얹인 닮른닿. 먀찔 몽씰한 뭏객쿠름쳐렴 한껐 풉풂령써 낮온눈뎀 잎벴 넣우면 쑨쉭칸웨 샬쌀 뇩눈댜. 닳음옘 뿌삵눼 냇령요면 블런찌는 억귀셔 콕 먹겨악겟타. 뮈린 챵갸 촉 옛약운 퓔쑤. 객쒸릴 탔쇼 촙쿄 씨셜뤼 놉훌퇸 뿌뿐닢 잇언 별 4께 추러타 호텔 윅칩, 젼먕, 셥피쓱 북뿌녜셔 념뭏 많쪽순렵귀웨 별 5깨룰 앉 줆 쑥캬 엎따. ㅎㅎ\n",
      "🛠️ [디버깅] 예측된 토큰 ID: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "🛠️ [디버깅] 복원된 문장: <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "복구 진행 중: 100%|██████████| 2/2 [01:49<00:00, 54.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 `submission.csv` 파일이 생성되었습니다!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_path = \"datas/test.csv\"\n",
    "submission_path = \"datas/submission.csv\"\n",
    "model_save_path = \"trained_decoder.pth\"\n",
    "\n",
    "\n",
    "pretrained_decoder = torch.load(\"trained_decoder_full.pth\")\n",
    "pretrained_decoder.to(device)\n",
    "pretrained_decoder.eval()\n",
    "\n",
    "\n",
    "# ✅ 테스트 데이터 로드\n",
    "df_test = pd.read_csv(test_path).sample(2)\n",
    "test_input = df_test['input']\n",
    "\n",
    "# ✅ 복원된 문장 저장 리스트\n",
    "reconstructed_outputs = []\n",
    "\n",
    "# ✅ Auto-regressive 방식에서 속도 최적화\n",
    "with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "    for input_text in tqdm(test_input, desc=\"복구 진행 중\"):\n",
    "        # 🔹 난독화된 문장 임베딩 벡터 생성\n",
    "        input_emb = get_encodedKr_emb_vec([input_text], mybart_tokenizer, mybart_model, device).permute(1, 0, 2)\n",
    "\n",
    "        # 🔹 디코딩할 첫 번째 입력 토큰 설정 (시작 토큰 `<sos>`)\n",
    "        start_token_id = kobart_tokenizer.bos_token_id\n",
    "        end_token_id = kobart_tokenizer.eos_token_id\n",
    "        current_token_ids = torch.tensor([[start_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "        # 🔹 Greedy Decoding을 위한 토큰 생성\n",
    "        generated_tokens = []\n",
    "\n",
    "        # 🔹 최대 `batch_max_len` 길이만큼 반복\n",
    "        for _ in range(batch_max_len):\n",
    "            # 🔹 현재까지의 생성된 토큰을 `kobart_model`로 인코딩\n",
    "            output_emb = kobart_model(current_token_ids).encoder_last_hidden_state.permute(1, 0, 2)\n",
    "\n",
    "            # 🔹 `tgt_mask` 생성 (자동회귀 방식 적용)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(output_emb.size(0)).to(device)\n",
    "\n",
    "            # 🔹 디코더 예측 수행\n",
    "            decoder_output = pretrained_decoder(input_emb, output_emb, tgt_mask=tgt_mask)  # (seq_len, batch_size, vocab_size)\n",
    "\n",
    "            # 🔹 가장 확률 높은 토큰 선택\n",
    "            next_token_id = decoder_output.argmax(dim=-1)[-1, 0].item()  # 마지막 토큰 선택\n",
    "\n",
    "            # 🔹 생성된 토큰 저장\n",
    "            generated_tokens.append(next_token_id)\n",
    "\n",
    "            # 🔹 종료 조건: `<eos>` 토큰이 생성되면 중단\n",
    "            if next_token_id == end_token_id:\n",
    "                break\n",
    "\n",
    "            # 🔹 다음 입력으로 사용 (효율적으로 변환)\n",
    "            current_token_ids = torch.cat([current_token_ids, torch.tensor([[next_token_id]], dtype=torch.long).to(device)], dim=1)\n",
    "\n",
    "        # 🔹 토큰 ID를 한국어 문장으로 변환\n",
    "        reconstructed_text = kobart_tokenizer.decode(generated_tokens)\n",
    "\n",
    "        # 🔹 디버깅 정보 출력 (1개만 확인)\n",
    "        if len(reconstructed_outputs) < 1:\n",
    "            print(f\"\\n🛠️ [디버깅] 원본: {input_text}\")\n",
    "            print(f\"🛠️ [디버깅] 예측된 토큰 ID: {generated_tokens}\")\n",
    "            print(f\"🛠️ [디버깅] 복원된 문장: {reconstructed_text}\")\n",
    "\n",
    "        reconstructed_outputs.append(reconstructed_text)\n",
    "\n",
    "# ✅ `submission.csv` 생성\n",
    "df_submission = pd.DataFrame({\"ID\": df_test.index.map(lambda x: f\"TEST_{x:04d}\"), \"output\": reconstructed_outputs})\n",
    "df_submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(\"\\n🎉 `submission.csv` 파일이 생성되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1(난독화 텍스트) 임베딩 벡터 생성용 함수 (배치 단위 지원)\n",
    "def get_encodedKr_emb_vec(input_texts, mybart_tokenizer, mybart_model, device, max_length=1026):\n",
    "    '''\n",
    "    mybart_model는 eval() 모드여야 한다.\n",
    "    input_texts: List of input strings (배치 단위)\n",
    "    '''\n",
    "    input_ids_batch = []\n",
    "    attention_mask_batch = []\n",
    "\n",
    "    for input_text in input_texts:\n",
    "        input_encoded = mybart_tokenizer.encode(input_text)\n",
    "        input_ids = input_encoded.ids[:max_length]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        pad_id = mybart_tokenizer.token_to_id(\"<pad>\")\n",
    "        input_ids += [pad_id] * (max_length - len(input_ids))\n",
    "        attention_mask += [0] * (max_length - len(attention_mask))\n",
    "\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "\n",
    "    # (batch_size, seq_len) 형태로 변환\n",
    "    input_ids = torch.tensor(input_ids_batch, dtype=torch.long).to(device)\n",
    "    attention_mask = torch.tensor(attention_mask_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    input_emb = mybart_model.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "    return input_emb  # (batch_size, seq_len, hidden_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
