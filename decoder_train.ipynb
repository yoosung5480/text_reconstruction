{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BartModel, BartTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.amp import autocast, GradScaler\n",
    "from transformers import BartModel\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "from transformers import BartConfig, BartModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전학습된 koBart, train_encoder 가져오기.\n",
    "\n",
    "샘플 텍스트를 각각 1줄씩 넣어보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "kobart_tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "kobart_model = AutoModel.from_pretrained(\"gogamza/kobart-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobart_tokenizer.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1026"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobart_model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 1024, 768])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sample_korean = '별 한 개도 아깝다.'\n",
    "output_enc = kobart_tokenizer.encode(sample_korean, return_tensors=\"pt\", padding=\"max_length\", max_length=max_len, truncation=True)\n",
    "print(output_enc.shape)\n",
    "\n",
    "output_enb = kobart_model(output_enc)\n",
    "print(output_enb.encoder_last_hidden_state.shape)\n",
    "print(output_enb.encoder_hidden_states)\n",
    "print(output_enb.encoder_attentions)\n",
    "\n",
    "# 사전정의된 kobart의 출력형태 (batch_size, max_lenth, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"tokenizers/BPE_tokenizer_50000_aug.json\"\n",
    "mybart_tokenizer = Tokenizer.from_file(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BART 인코더 가중치 로드 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95161/1573616973.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "d_model=768\n",
    "encoder_layers=4\n",
    "encoder_attention_heads=8\n",
    "\n",
    "config = BartConfig(\n",
    "    vocab_size=mybart_tokenizer.get_vocab_size(),\n",
    "    d_model=d_model,\n",
    "    encoder_layers=encoder_layers,\n",
    "    encoder_attention_heads=encoder_attention_heads,\n",
    "    max_position_embeddings=max_len\n",
    ")\n",
    "\n",
    "# BART 모델 설정\n",
    "bart_model = BartModel(config)  # BART 모델 생성\n",
    "\n",
    "# 저장된 textEncoder 모델 가중치 불러오기\n",
    "model_path = \"trained_encoder3.pth\"\n",
    "state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "# BART 모델의 인코더 부분에만 가중치 로드\n",
    "bart_model.encoder.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(\"✅ BART 인코더 가중치 로드 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "sample_encoded_text = '별 한 게토 았깝땀.'\n",
    "\n",
    "input_text = sample_encoded_text\n",
    "input_encoded = mybart_tokenizer.encode(input_text)\n",
    "input_ids = input_encoded.ids[:max_len]\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "pad_id = mybart_tokenizer.token_to_id(\"<pad>\")\n",
    "input_ids += [pad_id] * (max_len - len(input_ids))\n",
    "attention_mask += [0] * (max_len - len(attention_mask))\n",
    "\n",
    "# 트랜스포머 블럭의 input1, input2\n",
    "input_ids =  torch.tensor(input_ids, dtype=torch.long).unsqueeze(0)             # unsqueeze(0) 를 넣음으로써 [128] -> [1, 128] 형태로 변경되어 encoder에 사용가능해짐\n",
    "attention_mask =  torch.tensor(attention_mask, dtype=torch.long).unsqueeze(0)   # input_ids, attention_mask가 어텐션블럭에 필요하다.\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가 모드 설정\n",
    "bart_model.encoder.eval()\n",
    "\n",
    "# GPU 적용 (필요시)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 같은 디바이스로 연결\n",
    "bart_model.encoder.to(device)\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "\n",
    "outputs_enb2 = bart_model.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "outputs_enb2.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정의.\n",
    "\n",
    "'''\n",
    "데이터셋 param:\n",
    "\n",
    "데이터프레임저장위치,\n",
    "한국어tokenizer,\n",
    "난독화tokenizer,\n",
    "koBart,\n",
    "myBart,\n",
    "최대문자길이\n",
    "\n",
    "\n",
    "input : 한국어 -> output: 한국어 임베딩벡터\n",
    "intput : 난독화 -> output: 난독화 임베딩벡터 \n",
    "'''\n",
    "\n",
    "# 데이터셋 정의\n",
    "class DecoderDataset(Dataset):\n",
    "    '''\n",
    "     kobart, mybart 에 평가 모드를 먼저 설정해놓고 써야한다.\n",
    "    '''\n",
    "    def __init__(self, df_path, tokenizer_kobart, tokenizer_mybart, kobart, mybart, max_len, device):\n",
    "        df = pd.read_csv(df_path)\n",
    "        self.inputs = df[\"input\"].tolist()\n",
    "        self.outputs = df[\"output\"].tolist()\n",
    "        self.tokenizer_kobart = tokenizer_kobart\n",
    "        self.tokenizer_mybart = tokenizer_mybart\n",
    "        self.kobart = kobart\n",
    "        self.mybart = mybart\n",
    "        self.max_len = max_len    # 1026길이\n",
    "        self.device = device\n",
    "\n",
    "        mybart.encoder.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        output_text = self.outputs[idx]\n",
    "\n",
    "        # 한국어(output) 임베딩벡터 구하기\n",
    "        output_enc = self.tokenizer_kobart.encode(output_text, return_tensors=\"pt\", padding=\"max_length\", max_length=self.max_len, truncation=True)\n",
    "        output_emb = self.kobart(output_enc).encoder_last_hidden_state\n",
    "\n",
    "        # 난독화(input) 임베딩벡터 구하기\n",
    "        input_encoded = self.tokenizer_mybart.encode(input_text)\n",
    "        input_ids = input_encoded.ids[:self.max_len]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        pad_id = self.tokenizer_mybart.token_to_id(\"<pad>\")\n",
    "        input_ids += [pad_id] * (self.max_len - len(input_ids))\n",
    "        attention_mask += [0] * (self.max_len - len(attention_mask))\n",
    "\n",
    "        # 트랜스포머 블럭의 input1, input2\n",
    "        input_ids =  torch.tensor(input_ids, dtype=torch.long).unsqueeze(0)             # unsqueeze(0) 를 넣음으로써 [128] -> [1, 128] 형태로 변경되어 encoder에 사용가능해짐\n",
    "        attention_mask =  torch.tensor(attention_mask, dtype=torch.long).unsqueeze(0)   # input_ids, attention_mask가 어텐션블럭에 필요하다.\n",
    "\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "        intput_emb = self.mybart.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        return intput_emb , output_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "datset_path = 'datas/decoder_augmentation.csv'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bart_model.encoder.eval()\n",
    "\n",
    "dataset = DecoderDataset(\n",
    "    datset_path, \n",
    "    tokenizer_kobart=kobart_tokenizer,\n",
    "    tokenizer_mybart=mybart_tokenizer,\n",
    "    kobart=kobart_model,\n",
    "    mybart=bart_model,\n",
    "    max_len=max_len,\n",
    "    device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2226,  0.1414,  0.4612,  ...,  0.1128, -0.4251, -0.8362],\n",
      "         [ 0.0705, -0.1337,  0.7889,  ..., -2.4905, -1.2811,  1.3559],\n",
      "         [ 0.7684,  1.4161,  1.2982,  ..., -1.0140, -0.1258, -0.7655],\n",
      "         ...,\n",
      "         [ 0.5929, -0.5442,  0.7925,  ...,  0.3226,  0.3040, -0.4593],\n",
      "         [ 1.2082,  0.2888,  0.5889,  ...,  1.3947,  0.2523, -1.6183],\n",
      "         [ 0.6539,  0.8406, -0.2320,  ...,  0.5439, -0.4446, -1.6106]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>) tensor([[[ 0.6659,  0.1054,  0.3323,  ..., -0.0635, -0.1365, -0.5542],\n",
      "         [ 0.9946, -0.1736, -0.4148,  ...,  0.3800, -0.1090, -0.3821],\n",
      "         [ 0.3742, -0.3210,  0.0760,  ..., -0.2923, -0.1173, -0.1381],\n",
      "         ...,\n",
      "         [ 0.8260, -0.5777, -0.2591,  ...,  0.4395,  0.0237, -0.3241],\n",
      "         [ 0.8489, -0.5402, -0.2760,  ...,  0.4580,  0.0178, -0.3220],\n",
      "         [ 0.8377, -0.5736, -0.2744,  ...,  0.4677,  0.0114, -0.3317]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "intput_emb , output_emb = dataset.__getitem__(1)\n",
    "print(intput_emb, output_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024, 768]), torch.Size([1, 1024, 768]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intput_emb.shape, output_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datset_path)\n",
    "inputs = df[\"input\"].tolist()\n",
    "outputs = df[\"output\"].tolist()\n",
    "kobart_tokens = [kobart_tokenizer.encode(x) for x in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobart_tokens = [kobart_tokenizer.encode(x) for x in outputs]\n",
    "\n",
    "# output_enc = self.tokenizer_kobart.encode(output_text, return_tensors=\"pt\", padding=\"max_length\", max_length=self.max_len, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33789"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 섧멍핥쟈닐 탯끎룐눈 녀뮤 퀼교... 야뭍툰 둠 변 닺씨 깍낄 싫훈 굣. 깸삥읊 20여 년 댜녁뵨 곧 중 쩨윌 귑푼 낙팠떤 곶.',\n",
       " '별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자니 댓글로는 너무 길고... 아무튼 두 번 다시 가길 싫은 곳. 캠핑을 20여 년 다녀본 곳 중 제일 기분 나빴던 곳.')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0], outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 디코더의 네트워크 구조 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionDecoder(nn.Module):\n",
    "    def __init__(self, kobart_tokenizer_vocab_size, hidden_dim=768, num_layers=6, num_heads=8, dropout=0.1):\n",
    "        super(CrossAttentionDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kobart_tokenizer_vocab_size = kobart_tokenizer_vocab_size\n",
    "\n",
    "        # Transformer Decoder Layer에 dropout 추가\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout 추가\n",
    "        self.fc_out = nn.Linear(hidden_dim * 4, kobart_tokenizer_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(input2.size(1)).to(input2.device)\n",
    "        decoder_output = self.decoder(input2, input1, tgt_mask=tgt_mask)\n",
    "\n",
    "        x = self.fc1(decoder_output)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # Dropout 적용\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return self.softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 에포크 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95161/2809896680.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"trained_encoder3.pth\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BART 인코더 가중치 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BART 모델 아키텍처 로드 (전체 모델이 아니라 encoder만 사용)\n",
    "bart_model = BartModel.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# 저장된 가중치 로드\n",
    "state_dict = torch.load(\"trained_encoder3.pth\", map_location=\"cpu\")\n",
    "\n",
    "# 가중치를 encoder 부분만 로드\n",
    "bart_model.encoder.load_state_dict(state_dict, strict=False)  # strict=False 옵션 추가\n",
    "\n",
    "# GPU 적용\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bart_model.encoder.to(device)\n",
    "\n",
    "# 평가 모드 설정\n",
    "bart_model.encoder.eval()\n",
    "\n",
    "print(\"✅ BART 인코더 가중치 로드 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'txt_reconstruction/decoder_augmentation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[1;32m     11\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtxt_reconstruction/decoder_augmentation.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 토크나이저 로드\u001b[39;00m\n\u001b[1;32m     15\u001b[0m kobart_tokenizer \u001b[38;5;241m=\u001b[39m BartTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'txt_reconstruction/decoder_augmentation.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BartModel, BartTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"txt_reconstruction/decoder_augmentation.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 토크나이저 로드\n",
    "kobart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "with open(\"txt_reconstruction/tokenizers/BPE_tokenizer_50000_aug.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mybart_tokenizer = json.load(f)\n",
    "\n",
    "# 모델 로드\n",
    "kobart_model = BartModel.from_pretrained(\"facebook/bart-large\")  # koBART 모델 로드 (경로 변경 필요)\n",
    "mybart_encoder = BartModel.from_pretrained(\"facebook/bart-large\")  # myBART 모델 (경로 변경 필요)\n",
    "mybart_encoder.load_state_dict(torch.load(\"txt_reconstruction/trained_encoder.pth\"))\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 크로스 어텐션 기반 디코더\n",
    "class CrossAttentionDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, num_layers=6, num_heads=8):\n",
    "        super(CrossAttentionDecoder, self).__init__()\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, len(kobart_tokenizer))  # Vocab size\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(input2.size(1)).to(input2.device)\n",
    "        decoder_output = self.decoder(input2, input1, tgt_mask=tgt_mask)\n",
    "        output = self.fc_out(decoder_output)\n",
    "        return self.softmax(output)\n",
    "\n",
    "# 모델 초기화\n",
    "decoder_model = CrossAttentionDecoder().to(device)\n",
    "optimizer = optim.AdamW(decoder_model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()  # FP16 지원\n",
    "\n",
    "# 데이터셋 정의\n",
    "class DecoderDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer_kobart, tokenizer_mybart):\n",
    "        self.inputs = df[\"input\"].tolist()\n",
    "        self.outputs = df[\"output\"].tolist()\n",
    "        self.tokenizer_kobart = tokenizer_kobart\n",
    "        self.tokenizer_mybart = tokenizer_mybart\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        output_text = self.outputs[idx]\n",
    "\n",
    "        # 토큰화\n",
    "        input_enc = self.tokenizer_mybart.encode(input_text, return_tensors=\"pt\", padding=\"max_length\", max_length=128, truncation=True)\n",
    "        output_enc = self.tokenizer_kobart.encode(output_text, return_tensors=\"pt\", padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "        return input_enc.squeeze(0), output_enc.squeeze(0)\n",
    "\n",
    "# 데이터 로더 정의\n",
    "batch_size = 8  # GPU 메모리를 고려하여 조정\n",
    "dataset = DecoderDataset(df, kobart_tokenizer, mybart_tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# 학습 루프\n",
    "def train_decoder(dataloader, model, optimizer, loss_fn, epochs=5, accumulation_steps=4):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        for step, (input1, input2) in enumerate(dataloader):\n",
    "            input1, input2 = input1.to(device), input2.to(device)\n",
    "\n",
    "            with autocast():  # Mixed Precision Training 적용\n",
    "                output = model(input1, input2)\n",
    "                loss = loss_fn(output.view(-1, output.size(-1)), input2.view(-1))\n",
    "                loss = loss / accumulation_steps  # Gradient Accumulation 적용\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 학습 시작\n",
    "train_decoder(dataloader, decoder_model, optimizer, loss_fn, epochs=5, accumulation_steps=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
