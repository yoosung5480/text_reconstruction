{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.amp as amp\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import BartModel\n",
    "from transformers import BartModel, BartTokenizer\n",
    "from transformers import BartConfig, BartModel\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¶”ì²œ num_workers ê°’: 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "num_workers = os.cpu_count() // 2  # CPU ê°œìˆ˜ì˜ ì ˆë°˜ ì‚¬ìš© (ë³´í†µ ìµœì )\n",
    "print(f\"ì¶”ì²œ num_workers ê°’: {num_workers}\")\n",
    "\n",
    "# í˜„ì¬ í† í¬ë‚˜ì´ì € ë³‘ì—´í™”ì™€, num_workersë¥¼ ì‚¬ìš©í•˜ëŠ” ë³‘ë ¬í™”ê°€ ì¶©ë™ì¼ ì¼ìœ¼í‚´ìœ¼ë¡œ, í† í¬ë‚˜ì´ì € ë³‘ë ¬í™” ê¸°ëŠ¥ì„ ì œê±°í•˜ê³ ì‹œëŸ.\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‚¬ì „í•™ìŠµëœ koBart, train_encoder ê°€ì ¸ì˜¤ê¸°.\n",
    "\n",
    "ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¥¼ ê°ê° 1ì¤„ì”© ë„£ì–´ë³´ê¸°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°ì´í„°ì…‹ ì •ì˜í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ì •ì˜\n",
    "class DecoderDataset(Dataset):\n",
    "    def __init__(self, df_path):\n",
    "        df = pd.read_csv(df_path)\n",
    "        self.inputs = df[\"input\"].tolist()\n",
    "        self.outputs = df[\"output\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        output_text = self.outputs[idx]\n",
    "        return input_text, output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kobart, kobart_tokenizer ê°€ì ¸ì˜¤ëŠ” ì½”ë“œ\n",
    "def get_kobart_and_tokenizer():\n",
    "    kobart_tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "    kobart_model = AutoModelForSeq2SeqLM.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "    return kobart_tokenizer, kobart_model\n",
    "\n",
    "\n",
    "# mybart, mybart_tokenizer ê°€ì ¸ì˜¤ëŠ” ì½”ë“œ\n",
    "def get_mybart_and_tokenizer(tokenizer_path, model_path, model_config):\n",
    "    mybart_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "    # BART ëª¨ë¸ ì„¤ì •\n",
    "    mybart = BartModel(model_config)  # BART ëª¨ë¸ ìƒì„±\n",
    "\n",
    "    # ì €ì¥ëœ textEncoder ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "    state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "    # BART ëª¨ë¸ì˜ ì¸ì½”ë” ë¶€ë¶„ì—ë§Œ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "    mybart.encoder.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    print(\"âœ… BART ì¸ì½”ë” ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    return mybart_tokenizer, mybart\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1(ë‚œë…í™” í…ìŠ¤íŠ¸) ì„ë² ë”© ë²¡í„° ìƒì„±ìš© í•¨ìˆ˜ (ë°°ì¹˜ ë‹¨ìœ„ ì§€ì›)\n",
    "def get_encodedKr_emb_vec(input_texts, mybart_tokenizer, mybart_model, device, max_length=1026):\n",
    "    '''\n",
    "    mybart_modelëŠ” eval() ëª¨ë“œì—¬ì•¼ í•œë‹¤.\n",
    "    input_texts: List of input strings (ë°°ì¹˜ ë‹¨ìœ„)\n",
    "    '''\n",
    "    input_ids_batch = []\n",
    "    attention_mask_batch = []\n",
    "\n",
    "    for input_text in input_texts:\n",
    "        input_encoded = mybart_tokenizer.encode(input_text)\n",
    "        input_ids = input_encoded.ids[:max_length]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        pad_id = mybart_tokenizer.token_to_id(\"<pad>\")\n",
    "        input_ids += [pad_id] * (max_length - len(input_ids))\n",
    "        attention_mask += [0] * (max_length - len(attention_mask))\n",
    "\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "\n",
    "    # (batch_size, seq_len) í˜•íƒœë¡œ ë³€í™˜\n",
    "    input_ids = torch.tensor(input_ids_batch, dtype=torch.long).to(device)\n",
    "    attention_mask = torch.tensor(attention_mask_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    input_emb = mybart_model.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "    return input_emb  # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "# input2(í•œêµ­ì–´ í…ìŠ¤íŠ¸) ì„ë² ë”© ë²¡í„° ìƒì„±ìš© í•¨ìˆ˜ (ë°°ì¹˜ ë‹¨ìœ„ ì§€ì›)\n",
    "def get_korean_emb_vec(output_texts, kobart_tokenizer, kobart_model, device, max_length=1026):\n",
    "    '''\n",
    "    kobart_modelëŠ” eval() ëª¨ë“œì—¬ì•¼ í•œë‹¤.\n",
    "    output_texts: List of output strings (ë°°ì¹˜ ë‹¨ìœ„)\n",
    "    '''\n",
    "    output_ids_batch = kobart_tokenizer(output_texts, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                        max_length=max_length, truncation=True)[\"input_ids\"].to(device)\n",
    "\n",
    "    output_emb = kobart_model(output_ids_batch).encoder_last_hidden_state\n",
    "    return output_emb  # (batch_size, seq_len, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë””ì½”ë”ì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 kobart_tokenizer_vocab_size, \n",
    "                 hidden_dim=768, \n",
    "                 num_layers=6, \n",
    "                 num_heads=8, \n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super(CrossAttentionDecoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kobart_tokenizer_vocab_size = kobart_tokenizer_vocab_size\n",
    "\n",
    "        # Transformer Decoder Layerì— dropout ì¶”ê°€\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim * 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout ì¶”ê°€\n",
    "        self.fc_out = nn.Linear(hidden_dim * 4, kobart_tokenizer_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input1, input2, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        input1: ë‚œë…í™”ëœ í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¡í„° (Key, Value) -> (batch_size, seq_len, hidden_dim)\n",
    "        input2: ë³µì›ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¡í„° (Query) -> (batch_size, seq_len, hidden_dim)\n",
    "        tgt_mask: (batch_size, seq_len, seq_len) í¬ê¸°ì˜ ë§ˆìŠ¤í¬ í…ì„œ (ì™¸ë¶€ì—ì„œ ì œê³µ)\n",
    "        \"\"\"\n",
    "        decoder_output = self.decoder(input2, input1, tgt_mask=tgt_mask)\n",
    "\n",
    "        x = self.fc1(decoder_output)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # Dropout ì ìš©\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return self.softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í›ˆë ¨ ì—í¬í¬ ì‘ì„±í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì¼ë‹¨ 1ê°œì˜ ë°°ì¹˜ë§Œ í˜„ì¬ ë„¤íŠ¸ì›Œí¬ì— ë„£ì–´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹, ë°ì´í„°ë¡œë” ì •ì˜\n",
    "datset_path = 'datas/decoder_augmentation.csv'\n",
    "decoder_dataset = DecoderDataset(df_path= datset_path)\n",
    "dataloader = DataLoader(\n",
    "    decoder_dataset, \n",
    "    batch_size=2, \n",
    "    shuffle=True, \n",
    "    num_workers=4, \n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "dataiter = iter(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/tmp/ipykernel_150021/1922565671.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BART ì¸ì½”ë” ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ì „í•™ìŠµëœ í† í¬ë‚˜ì´ì €, BARTì¸ì½”ë” ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "# ì €ì¥í•´ë†“ì€ ëª¨ë¸ì˜ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì™€ ë™ì¼í•´ì•¼í•œë‹¤.\n",
    "model_config = BartConfig(\n",
    "    vocab_size=50000,\n",
    "    d_model=768,\n",
    "    encoder_layers=4,\n",
    "    encoder_attention_heads=8,\n",
    "    max_position_embeddings=1026,\n",
    ")\n",
    "tokenizer_path = \"tokenizers/BPE_tokenizer_50000_aug.json\"\n",
    "model_path = \"trained_encoder3.pth\"\n",
    "\n",
    "# í† í¬ë‚˜ì´ì €, ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œ.\n",
    "kobart_tokenizer, kobart_model = get_kobart_and_tokenizer()\n",
    "mybart_tokenizer, mybart_model = get_mybart_and_tokenizer(tokenizer_path, model_path, model_config)\n",
    "\n",
    "# kobart_tokenizer.max_len_single_sentence, kobart_tokenizer.vocab_size # (1000000000000000019884624838656, 30000)\n",
    "# kobart_model.config.d_model, kobart_model.config.max_position_embeddings # (768, 1026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BartForConditionalGeneration(\n",
       "   (model): BartModel(\n",
       "     (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "     (encoder): BartEncoder(\n",
       "       (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "       (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "       (layers): ModuleList(\n",
       "         (0-5): 6 x BartEncoderLayer(\n",
       "           (self_attn): BartSdpaAttention(\n",
       "             (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (decoder): BartDecoder(\n",
       "       (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
       "       (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "       (layers): ModuleList(\n",
       "         (0-5): 6 x BartDecoderLayer(\n",
       "           (self_attn): BartSdpaAttention(\n",
       "             (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartSdpaAttention(\n",
       "             (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "           (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       " ),\n",
       " BartModel(\n",
       "   (shared): BartScaledWordEmbedding(50000, 768, padding_idx=1)\n",
       "   (encoder): BartEncoder(\n",
       "     (embed_tokens): BartScaledWordEmbedding(50000, 768, padding_idx=1)\n",
       "     (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "     (layers): ModuleList(\n",
       "       (0-3): 4 x BartEncoderLayer(\n",
       "         (self_attn): BartSdpaAttention(\n",
       "           (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         )\n",
       "         (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (activation_fn): GELUActivation()\n",
       "         (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "         (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "         (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "     )\n",
       "     (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (decoder): BartDecoder(\n",
       "     (embed_tokens): BartScaledWordEmbedding(50000, 768, padding_idx=1)\n",
       "     (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
       "     (layers): ModuleList(\n",
       "       (0-11): 12 x BartDecoderLayer(\n",
       "         (self_attn): BartSdpaAttention(\n",
       "           (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         )\n",
       "         (activation_fn): GELUActivation()\n",
       "         (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (encoder_attn): BartSdpaAttention(\n",
       "           (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         )\n",
       "         (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
       "         (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
       "         (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       )\n",
       "     )\n",
       "     (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ë“¤ì˜ ëª¨ë“œ ì„¤ì • ë° device ì„¤ì •.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ë“¤ì€ í›ˆë ¨íŒŒë¼ë¯¸í„°ë¡œ ì‚¬ìš© x\n",
    "kobart_model.eval(), mybart_model.eval()\n",
    "kobart_model.to(device), mybart_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossAttentionDecoder(\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc_out): Linear(in_features=3072, out_features=30000, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë””ì½”ë” ì •ì˜.\n",
    "mydecoder = CrossAttentionDecoder(\n",
    "    kobart_tokenizer_vocab_size=kobart_tokenizer.vocab_size,\n",
    "    hidden_dim=768,\n",
    "    num_layers=1,\n",
    "    num_heads=1,\n",
    "    dropout=0.1)\n",
    "\n",
    "mydecoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1026, 768]), torch.Size([2, 1026, 768]), torch.Size([1026, 2]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì˜ˆì‹œìš© ì¶œë ¥ì„ìœ¼ë¡œ, detachí•´ì„œ í›ˆë ¨ì— ì˜í–¥ì£¼ì§€ ì•Šë„ë¡í•œë‹¤.\n",
    "batch = next(dataiter)\n",
    "input_texts , output_texts = batch\n",
    "\n",
    "sample_output_emb = get_korean_emb_vec(output_texts, kobart_tokenizer, kobart_model, device).detach()\n",
    "sample_input_emb = get_encodedKr_emb_vec(input_texts, mybart_tokenizer, mybart_model, device).detach()\n",
    "\n",
    "\n",
    "ground_truth_token_ids = kobart_tokenizer(output_texts, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                          max_length=1026, truncation=True)[\"input_ids\"].T.detach()\n",
    "\n",
    "\n",
    "sample_output_emb.shape, sample_input_emb.shape, ground_truth_token_ids.shape # (torch.Size([1, 1026, 768]), torch.Size([1, 1026, 768]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1026, 2, 768]) torch.Size([1026, 2, 768]) torch.Size([1026, 1026])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1026, 2, 30000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì˜ ê²°ê³¼ ì¶œë ¥.\n",
    "batch_size = 2\n",
    "batch_max_len = 1026\n",
    "for inputs, outputs in dataloader:\n",
    "    # ğŸ”¥ ë°°ì¹˜ ë‚´ ìµœëŒ€ `seq_len`ì„ ì°¾ê¸°\n",
    "    input_emb = get_encodedKr_emb_vec(input_texts=inputs, \n",
    "                                      mybart_tokenizer=mybart_tokenizer,\n",
    "                                      mybart_model=mybart_model,\n",
    "                                      device=device).permute(1, 0, 2)\n",
    "    output_emb = get_korean_emb_vec(output_texts=outputs,\n",
    "                                    kobart_tokenizer=kobart_tokenizer,\n",
    "                                    kobart_model=kobart_model,\n",
    "                                    device=device).permute(1, 0, 2)\n",
    "    # ğŸ”¥ `tgt_mask`ë¥¼ batch_max_len ê¸°ì¤€ìœ¼ë¡œ ìƒì„±\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(batch_max_len).to(device)\n",
    "\n",
    "    print(input_emb.shape, output_emb.shape, tgt_mask.shape)\n",
    "    output = mydecoder(input_emb, output_emb, tgt_mask=tgt_mask).detach()\n",
    "    break\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = output.reshape(-1, output.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2052, 30000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1026, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([658, 658], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ”¹ `output`ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í† í° ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°\n",
    "predicted_token_ids = output_emb.argmax(dim=-1)  # (1026, 2)\n",
    "print(predicted_token_ids.shape)\n",
    "predicted_token_ids[1025]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë³µì›ëœ ë¬¸ì¥ê³¼ ì •ë‹µ ë¹„êµí•˜ì—¬ ì˜¤ì°¨ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í† í° ì •í™•ë„: 0.0000 (0/2052)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ì •ë‹µ í† í° ë³€í™˜ (batch_size ë‹¨ìœ„ë¡œ `outputs`ì„ í† í°í™”)\n",
    "ground_truth_token_ids = kobart_tokenizer(outputs, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                         max_length=batch_max_len, truncation=True)[\"input_ids\"].T.to(device)\n",
    "# ğŸ”¹ `.T`ë¥¼ ì‚¬ìš©í•˜ì—¬ `batch_size`ë¥¼ ë‘ ë²ˆì§¸ ì°¨ì›ìœ¼ë¡œ ë§ì¶¤ (max_len, batch_size)\n",
    "\n",
    "# ğŸ”¹ ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µê°’ ë¹„êµ\n",
    "correct = (predicted_token_ids == ground_truth_token_ids).sum().item()  # ë§ì¶˜ í† í° ê°œìˆ˜\n",
    "total = batch_max_len * batch_size  # ì „ì²´ í† í° ê°œìˆ˜\n",
    "\n",
    "# ğŸ”¹ ì •í™•ë„ ê³„ì‚°\n",
    "accuracy = correct / total\n",
    "print(f\"âœ… í† í° ì •í™•ë„: {accuracy:.4f} ({correct}/{total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ ë³µì›ëœ ë¬¸ì¥ 1: PÎ³Î³Î³Î³Î³Î³<unused50>Î³P<unused50><unused50><unused50><unused50><unused50>PPPPPÎ³PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPÎ³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPÎ³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP1ÊˆÊˆÊˆÊˆ11ÊˆÊˆÉ¤É¤É¤11ÊˆÊˆÊˆ111ÊˆÉ¤É¤É¤É¤É¤ÊˆÉ¤É¤111ÊˆÊˆÉ¤111ÊˆÊˆÉ¤Æ°1Êˆ<sys><sys><sys>111<sys><sys>Æ°Æ°Æ°Æ°Æ°Æ°Æ°Æ°1<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡É¤É¤É¤É¤É¤É¤É¤É¤É¤É¤ÊˆÊˆÊˆÊˆÉ¤É¤ÊˆÊˆÊˆÉ¤É¤É¤É¤É¤Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Î³Î³Î³Î³Î³Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Î³Ï‡Ï‡Î³Ï‡Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³\n",
      "ğŸ”¹ ë³µì›ëœ ë¬¸ì¥ 2: PÎ³Î³Î³Î³PPPPPÎ³PP<unused50><unused50>Î³<unused50>PP<unused50>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPÎ³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPÎ³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP1P>;->Î111Æ°Æ°Æ°Æ°Æ°1Æ°Æ°<sys>1111Æ°ÂÉ¤111Æ°Æ°É¤111>;->ÂĞ¥11Æ°Æ°Æ°Æ°Æ°Æ°Æ°Æ°Æ°Æ°11Æ°Æ°Æ°Æ°Æ°Æ°Æ°Æ°Æ°<unused50>Æ°<unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50><unused50>Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Å Å ÊˆÊˆÊˆÉ¤ÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÊˆÎ³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³<unused50><unused50><unused50><unused50><unused50><unused50>Î³Î³Î³Î³Î³Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Î³Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Ï‡Î³Î³Ï‡Ï‡Î³Ï‡Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³Î³\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³µì›\n",
    "decoded_texts = []\n",
    "batch_size = predicted_token_ids.shape[1]  # ë°°ì¹˜ í¬ê¸°\n",
    "\n",
    "for i in range(batch_size):  # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë°˜ë³µ\n",
    "    token_ids = predicted_token_ids[:, i].tolist()  # ğŸ”¥ (max_len,) í˜•íƒœë¡œ ë³€í™˜\n",
    "    decoded_text = kobart_tokenizer.decode(token_ids)  # ğŸ”¥ ê°œë³„ ë¬¸ì¥ ë³µì›\n",
    "    decoded_texts.append(decoded_text)\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ í™•ì¸\n",
    "for i, text in enumerate(decoded_texts):\n",
    "    print(f\"ğŸ”¹ ë³µì›ëœ ë¬¸ì¥ {i+1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ ì‹¤ì œë¬¸ì¥ ë¬¸ì¥ 1: ë§¤íŠ¸ë¦¬ìŠ¤ì™€ ë² ê°œì—ì„œ ì—„ì²­ë‚œ ê³°íŒ¡ì´ ëƒ„ìƒˆê°€ ë‚©ë‹ˆë‹¤. ë˜ ë‹¤ë¥¸ ë² ê°œì—ì„œëŠ” ì˜¤ë˜ëœ ë¨¸ë¦¬ì¹´ë½ ëƒ„ìƒˆê°€ ë‚©ë‹ˆë‹¤. ì¹¨êµ¬ ê´€ë¦¬ê°€ ì „í˜€ ì•ˆ í•˜ëŠ” ë“¯í•©ë‹ˆë‹¤. ëˆì´ ì•„ê¹ìŠµë‹ˆë‹¤.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "ğŸ”¹ ì‹¤ì œë¬¸ì¥ ë¬¸ì¥ 2: ì¹œì ˆí•˜ì§€ ì•Šì€ ì„œë¹„ìŠ¤ëŠ” ì—†ì—ˆìŒ. ìµœì†Œ 0ì˜ ì„œë¹„ìŠ¤ë¥¼ ë°›ì•˜ê³ , ìµœëŒ€ëŠ” í•œë„ê°€ ëë„ ì—†ì—ˆìœ¼ë‚˜, ì´ê²ƒì €ê²ƒ ì•Œê³  ì„œì¹­í•´ì„œ ê°€ì•¼ ì„œë¹„ìŠ¤ë¥¼ ë°›ì„ ìˆ˜ ìˆë‹¤ëŠ” ì ê³¼ í• ì¸ ì—†ì´ ê°ˆ ë§Œí•œ ê°€ê²©ëŒ€ì¸ì§€ëŠ”..? ì£¼ë³€ì— ì´ê²ƒì €ê²ƒ ë§ì•„ì„œ ì²´ë ¥ì„ ê³ ë ¤í•œ ìˆ™ì†Œë¡œ ì •í•˜ê¸° ì¢‹ì•˜ìŒ.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³µì›\n",
    "decoded_texts = []\n",
    "batch_size = ground_truth_token_ids.shape[1]  # ë°°ì¹˜ í¬ê¸°\n",
    "\n",
    "for i in range(batch_size):  # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë°˜ë³µ\n",
    "    token_ids = ground_truth_token_ids[:, i].tolist()  # ğŸ”¥ (max_len,) í˜•íƒœë¡œ ë³€í™˜\n",
    "    decoded_text = kobart_tokenizer.decode(token_ids) # ğŸ”¥ ê°œë³„ ë¬¸ì¥ ë³µì›\n",
    "    decoded_texts.append(decoded_text)\n",
    "\n",
    "# ğŸ”¹ ê²°ê³¼ í™•ì¸\n",
    "for i, text in enumerate(decoded_texts):\n",
    "    print(f\"ğŸ”¹ ì‹¤ì œë¬¸ì¥ ë¬¸ì¥ {i+1}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 100/16895 [00:50<2:21:13,  1.98it/s, accuracy=0, loss=0.0666]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Epoch 1/1 - Loss: 0.0079 | Accuracy: 0.0000\n",
      "\n",
      "ğŸ‰ í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ê³¼ í•™ìŠµ ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµ ì„¤ì •\n",
    "epochs = 1\n",
    "batch_size = 2\n",
    "batch_max_len = 1026\n",
    "learning_rate = 5e-5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(mydecoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Mixed Precision Training ì„¤ì •\n",
    "scaler = GradScaler()\n",
    "\n",
    "# í•™ìŠµ ê¸°ë¡ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "# í•™ìŠµ ë£¨í”„ ì‹œì‘\n",
    "for epoch in range(epochs):\n",
    "    mydecoder.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    tqdm_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for step, (inputs, outputs) in tqdm_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ğŸ”¹ ë°°ì¹˜ ë‚´ ìµœëŒ€ `seq_len` ì°¾ê¸°\n",
    "        input_emb = get_encodedKr_emb_vec(inputs, mybart_tokenizer, mybart_model, device).permute(1, 0, 2)  # (torch.Size([2, 1026, 768])\n",
    "        output_emb = get_korean_emb_vec(outputs, kobart_tokenizer, kobart_model, device).permute(1, 0, 2)   # (torch.Size([2, 1026, 768])\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(batch_max_len).to(device)                 # torch.Size([1026, 1026])\n",
    "\n",
    "        # ğŸ”¹ Mixed Precision Training ì ìš©\n",
    "        with autocast(device_type=device.type):\n",
    "            decoder_output = mydecoder(input_emb, output_emb, tgt_mask=tgt_mask)  # decoder_output: (seq_len, batch_size, vocab_size)\n",
    "            \n",
    "            ground_truth_token_ids = kobart_tokenizer(outputs, return_tensors=\"pt\", padding=\"max_length\",\n",
    "                                                    max_length=batch_max_len, truncation=True)[\"input_ids\"].T.to(device)  # (seq_len, batch_size)\n",
    "            \n",
    "\n",
    "            # input1: (batch_size*max_len, vocab_size) ì˜ˆì¸¡í•œ í† í° ë¡œì§“ê°’, input2: (batch_size*max_len) gtí† í° idë²ˆí˜¸\n",
    "            loss = loss_fn(decoder_output.reshape(-1, decoder_output.size(-1)), ground_truth_token_ids.reshape(-1))\n",
    "\n",
    "        \n",
    "        # ğŸ”¹ Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # ğŸ”¹ ì†ì‹¤ ë° ì •ë‹µë¥  ê³„ì‚°\n",
    "        total_loss += loss.item()\n",
    "        correct = (predicted_token_ids == ground_truth_token_ids).sum().item()  # ë§ì¶˜ í† í° ê°œìˆ˜\n",
    "        total_correct += correct\n",
    "        total_tokens += batch_max_len * batch_size\n",
    "\n",
    "        accuracy = correct / (batch_max_len * batch_size)\n",
    "\n",
    "        # ğŸ”¹ tqdm ì—…ë°ì´íŠ¸\n",
    "        tqdm_bar.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
    "\n",
    "        if step == 100:\n",
    "            break\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # ğŸ”¹ ì—í¬í¬ë‹¹ í‰ê·  ì†ì‹¤ ë° ì •í™•ë„ ê¸°ë¡\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_accuracy = total_correct / total_tokens\n",
    "\n",
    "    loss_history.append(epoch_loss)\n",
    "    accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "    print(f\"\\nâœ… Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} | Accuracy: {epoch_accuracy:.4f}\")\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ğŸ”¥ í›ˆë ¨ ì™„ë£Œ í›„ ì†ì‹¤ ë° ì •ë‹µë¥  ì €ì¥\n",
    "torch.save(mydecoder, \"trained_decoder_full.pth\")\n",
    "\n",
    "print(\"\\nğŸ‰ í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ê³¼ í•™ìŠµ ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_150021/2090858286.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_decoder = torch.load(\"trained_decoder_full.pth\")\n",
      "ë³µêµ¬ ì§„í–‰ ì¤‘:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:54<00:54, 54.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ› ï¸ [ë””ë²„ê¹…] ì›ë³¸: ê»™ì‰´ì—£ì¨ ë³¸ìœˆëˆˆ ë¶€ëŠ” ì²­ë¨ˆ ì¬ê¼°. í™…ë—„ë¦¼ ì˜®ë ˆí‡´ëŒœ í°ë‹‰êº„ ì ¼ì·Œì²™ì•ë£Œ ë‡¨íœ´í™•ë¼êµ ì‹¶ì„¦ì¼ ì˜œê»‘ì”¨ë€ëŠ” ì²¨ìš´ ì•ì‰½ì§•ë§, ì½´ë¤½ëˆˆ ë§µìš¹ ì²­ê±ºí•™ê¼ ì§§ íˆ‡ì–½ ì‡íƒ€. ì¥‘ê±´íˆ´ì« ì—ë…ëƒ ì…”ë¹›ìŠ¨ëˆˆ ë®¤ì© ì·¬ì©”í–ëŒœ. ë­„ì—ˆë½€ë‹«í†  ë§˜ë­¬ ë“¤ì—‡ë ê¼‡ìˆœ ë£¸ì„¶ë¹šìˆ˜ ë­¬ë‰¼. í–‰ë¬¾ëŒë¨¼ ê°›ì¶œ. ì¥‘êµ¼ì¹´ì¥ ë¨¸ì»¸í„´ ë‘ë©´ ì¸™ì˜ˆ ìœ„íˆë¡ ê³§êµ½ì° ëŒë¨¼ëˆˆ ì©Œìšº ë©±ì–µëµ¨ ë“£. ì ¼í­, ì´ˆê»™ê´€ì°¬, ìŒ”ìœ , ê½ƒê»˜ ë‘¥ìœŒ ì”¬ì…˜í•œ í—¤ìƒ¨ë¬¾ìœ„ ì©¡ë¨ˆ ë©±ê·€ ì¢‹ê¶¤ ë´ë¨¼ê³¼ ì–¼ì›ë ¤ìª„ ëƒ˜ìš˜ëŠ”ë—‘, ë¥¨ì—˜ì„ ì•ì”©ê¿€ë£Œ ì‰­ê»´ ë©±ê¹ íƒìœ…ëŒœ. ì‚ì˜, ì°í€¸, ê¹–ë›°ë´ ì–—ì´ ë¿Œì¥ í•™ê¼° ë§¡ë´ ì¢‹ëŒœ. ë¡­ì‚£ì¯©ì˜ ì˜³í„ìˆ ë´ëŠ” ì•¼ì´ë¥ì‰¿ í…ì—¡ì„¶ ë¸”ëŸ°ì·°ë¡¤ êµ°ëƒ˜ì˜ ì’œë¿Œ ì¸„ì²œ ì„ëš ë­¬ëˆ—ëŸ‰ íŒ¬ê»˜ìœ—ì¿ ë¥¼ ì‹ì»¤ ë¨¹ì˜€ëˆˆí…Œ, ëšœ ê±” ëŒœ ìº‰ì®¸! ìˆ˜í†„ìµì¿¤ëŠ” ìœ¡ì§ˆê³½ ê¾¸ì› ë„¨ëŠ” ì²­í† ë´ ë”± ì¢‹ì•—êµ, ëº€ê¼ìŠì¿ ëŠ” ë‚œìŒ©ìª„ì›€ í°ëˆˆ ì‚ì£¼ì—´ë£Œ í’ì·¸ê°¤ì² ë ´ ëƒìš˜ëŠ” ì—®ëˆ„ í¿ë˜¥ì˜ íœì¼•ìµë™ì™€ëˆˆ ì§œì–¹ì¸ ë‹®ë¥¸ë‹¿. ë¨€ì°” ëª½ì”°í•œ ë­ê°ì¿ ë¦„ì³ë ´ í•œê» í’‰í’‚ë ¹ì¨ ë‚®ì˜¨ëˆˆë€ ìë²´ ë„£ìš°ë©´ ì‘¨ì‰­ì¹¸ì›¨ ìƒ¬ìŒ€ ë‡©ëˆˆëŒœ. ë‹³ìŒì˜˜ ë¿Œì‚µëˆ¼ ëƒ‡ë ¹ìš”ë©´ ë¸”ëŸ°ì°ŒëŠ” ì–µê·€ì…” ì½• ë¨¹ê²¨ì•…ê²Ÿíƒ€. ë®ˆë¦° ì±µê°¸ ì´‰ ì˜›ì•½ìš´ í“”ì‘¤. ê°ì’¸ë¦´ íƒ”ì‡¼ ì´™ì¿„ ì”¨ì…œë¤¼ ë†‰í›Œí‡¸ ë¿Œë¿ë‹¢ ì‡ì–¸ ë³„ 4ê»˜ ì¶”ëŸ¬íƒ€ í˜¸í…” ìœ…ì¹©, ì ¼ë¨•, ì…¥í”¼ì“± ë¶ë¿Œë…œì…” ë…ë­ ë§ìª½ìˆœë µê·€ì›¨ ë³„ 5ê¹¨ë£° ì•‰ ì¤† ì‘¥ìº¬ ì—ë”°. ã…ã…\n",
      "ğŸ› ï¸ [ë””ë²„ê¹…] ì˜ˆì¸¡ëœ í† í° ID: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "ğŸ› ï¸ [ë””ë²„ê¹…] ë³µì›ëœ ë¬¸ì¥: <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ë³µêµ¬ ì§„í–‰ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:49<00:00, 54.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ `submission.csv` íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_path = \"datas/test.csv\"\n",
    "submission_path = \"datas/submission.csv\"\n",
    "model_save_path = \"trained_decoder.pth\"\n",
    "\n",
    "\n",
    "pretrained_decoder = torch.load(\"trained_decoder_full.pth\")\n",
    "pretrained_decoder.to(device)\n",
    "pretrained_decoder.eval()\n",
    "\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "df_test = pd.read_csv(test_path).sample(2)\n",
    "test_input = df_test['input']\n",
    "\n",
    "# âœ… ë³µì›ëœ ë¬¸ì¥ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "reconstructed_outputs = []\n",
    "\n",
    "# âœ… Auto-regressive ë°©ì‹ì—ì„œ ì†ë„ ìµœì í™”\n",
    "with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n",
    "    for input_text in tqdm(test_input, desc=\"ë³µêµ¬ ì§„í–‰ ì¤‘\"):\n",
    "        # ğŸ”¹ ë‚œë…í™”ëœ ë¬¸ì¥ ì„ë² ë”© ë²¡í„° ìƒì„±\n",
    "        input_emb = get_encodedKr_emb_vec([input_text], mybart_tokenizer, mybart_model, device).permute(1, 0, 2)\n",
    "\n",
    "        # ğŸ”¹ ë””ì½”ë”©í•  ì²« ë²ˆì§¸ ì…ë ¥ í† í° ì„¤ì • (ì‹œì‘ í† í° `<sos>`)\n",
    "        start_token_id = kobart_tokenizer.bos_token_id\n",
    "        end_token_id = kobart_tokenizer.eos_token_id\n",
    "        current_token_ids = torch.tensor([[start_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "        # ğŸ”¹ Greedy Decodingì„ ìœ„í•œ í† í° ìƒì„±\n",
    "        generated_tokens = []\n",
    "\n",
    "        # ğŸ”¹ ìµœëŒ€ `batch_max_len` ê¸¸ì´ë§Œí¼ ë°˜ë³µ\n",
    "        for _ in range(batch_max_len):\n",
    "            # ğŸ”¹ í˜„ì¬ê¹Œì§€ì˜ ìƒì„±ëœ í† í°ì„ `kobart_model`ë¡œ ì¸ì½”ë”©\n",
    "            output_emb = kobart_model(current_token_ids).encoder_last_hidden_state.permute(1, 0, 2)\n",
    "\n",
    "            # ğŸ”¹ `tgt_mask` ìƒì„± (ìë™íšŒê·€ ë°©ì‹ ì ìš©)\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(output_emb.size(0)).to(device)\n",
    "\n",
    "            # ğŸ”¹ ë””ì½”ë” ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "            decoder_output = pretrained_decoder(input_emb, output_emb, tgt_mask=tgt_mask)  # (seq_len, batch_size, vocab_size)\n",
    "\n",
    "            # ğŸ”¹ ê°€ì¥ í™•ë¥  ë†’ì€ í† í° ì„ íƒ\n",
    "            next_token_id = decoder_output.argmax(dim=-1)[-1, 0].item()  # ë§ˆì§€ë§‰ í† í° ì„ íƒ\n",
    "\n",
    "            # ğŸ”¹ ìƒì„±ëœ í† í° ì €ì¥\n",
    "            generated_tokens.append(next_token_id)\n",
    "\n",
    "            # ğŸ”¹ ì¢…ë£Œ ì¡°ê±´: `<eos>` í† í°ì´ ìƒì„±ë˜ë©´ ì¤‘ë‹¨\n",
    "            if next_token_id == end_token_id:\n",
    "                break\n",
    "\n",
    "            # ğŸ”¹ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© (íš¨ìœ¨ì ìœ¼ë¡œ ë³€í™˜)\n",
    "            current_token_ids = torch.cat([current_token_ids, torch.tensor([[next_token_id]], dtype=torch.long).to(device)], dim=1)\n",
    "\n",
    "        # ğŸ”¹ í† í° IDë¥¼ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜\n",
    "        reconstructed_text = kobart_tokenizer.decode(generated_tokens)\n",
    "\n",
    "        # ğŸ”¹ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ (1ê°œë§Œ í™•ì¸)\n",
    "        if len(reconstructed_outputs) < 1:\n",
    "            print(f\"\\nğŸ› ï¸ [ë””ë²„ê¹…] ì›ë³¸: {input_text}\")\n",
    "            print(f\"ğŸ› ï¸ [ë””ë²„ê¹…] ì˜ˆì¸¡ëœ í† í° ID: {generated_tokens}\")\n",
    "            print(f\"ğŸ› ï¸ [ë””ë²„ê¹…] ë³µì›ëœ ë¬¸ì¥: {reconstructed_text}\")\n",
    "\n",
    "        reconstructed_outputs.append(reconstructed_text)\n",
    "\n",
    "# âœ… `submission.csv` ìƒì„±\n",
    "df_submission = pd.DataFrame({\"ID\": df_test.index.map(lambda x: f\"TEST_{x:04d}\"), \"output\": reconstructed_outputs})\n",
    "df_submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(\"\\nğŸ‰ `submission.csv` íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1(ë‚œë…í™” í…ìŠ¤íŠ¸) ì„ë² ë”© ë²¡í„° ìƒì„±ìš© í•¨ìˆ˜ (ë°°ì¹˜ ë‹¨ìœ„ ì§€ì›)\n",
    "def get_encodedKr_emb_vec(input_texts, mybart_tokenizer, mybart_model, device, max_length=1026):\n",
    "    '''\n",
    "    mybart_modelëŠ” eval() ëª¨ë“œì—¬ì•¼ í•œë‹¤.\n",
    "    input_texts: List of input strings (ë°°ì¹˜ ë‹¨ìœ„)\n",
    "    '''\n",
    "    input_ids_batch = []\n",
    "    attention_mask_batch = []\n",
    "\n",
    "    for input_text in input_texts:\n",
    "        input_encoded = mybart_tokenizer.encode(input_text)\n",
    "        input_ids = input_encoded.ids[:max_length]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        pad_id = mybart_tokenizer.token_to_id(\"<pad>\")\n",
    "        input_ids += [pad_id] * (max_length - len(input_ids))\n",
    "        attention_mask += [0] * (max_length - len(attention_mask))\n",
    "\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "\n",
    "    # (batch_size, seq_len) í˜•íƒœë¡œ ë³€í™˜\n",
    "    input_ids = torch.tensor(input_ids_batch, dtype=torch.long).to(device)\n",
    "    attention_mask = torch.tensor(attention_mask_batch, dtype=torch.long).to(device)\n",
    "\n",
    "    input_emb = mybart_model.encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "    return input_emb  # (batch_size, seq_len, hidden_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
